{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc456178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from scipy import test\n",
    "import torch\n",
    "import torch.cuda\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import argparse\n",
    "import gc\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "import wandb\n",
    "\n",
    "from datasets import get_dataset\n",
    "from models.models import all_models\n",
    "\n",
    "from client import Client\n",
    "from utils import *\n",
    "\n",
    "import fedsnip_obj\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "def device_list(x):\n",
    "    if x == 'cpu':\n",
    "        return [x]\n",
    "    return [int(y) for y in x.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2442c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--single_shot_pruning'], dest='single_shot_pruning', nargs=0, const=True, default=False, type=None, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--eta', type=float, help='learning rate', default=0.01)\n",
    "parser.add_argument('--clients', type=int, help='number of clients per round', default=20)\n",
    "parser.add_argument('--rounds', type=int, help='number of global rounds', default=400)\n",
    "parser.add_argument('--epochs', type=int, help='number of local epochs', default=10)\n",
    "parser.add_argument('--dataset', type=str, choices=('mnist', 'emnist', 'cifar10', 'cifar100'),\n",
    "                    default='mnist', help='Dataset to use')\n",
    "parser.add_argument('--distribution', type=str, choices=('dirichlet', 'lotteryfl', 'iid', 'classic_iid'), default='dirichlet',\n",
    "                    help='how should the dataset be distributed?')\n",
    "parser.add_argument('--beta', type=float, default=0.1, help='Beta parameter (unbalance rate) for Dirichlet distribution')\n",
    "parser.add_argument('--total-clients', type=int, help='split the dataset between this many clients. Ignored for EMNIST.', default=400)\n",
    "parser.add_argument('--min-samples', type=int, default=0, help='minimum number of samples required to allow a client to participate')\n",
    "parser.add_argument('--samples-per-client', type=int, default=20, help='samples to allocate to each client (per class, for lotteryfl, or per client, for iid)')\n",
    "parser.add_argument('--prox', type=float, default=0, help='coefficient to proximal term (i.e. in FedProx)')\n",
    "\n",
    "parser.add_argument('--batch-size', type=int, default=32,\n",
    "                    help='local client batch size')\n",
    "parser.add_argument('--l2', default=0.001, type=float, help='L2 regularization strength')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='Local client SGD momentum parameter')\n",
    "parser.add_argument('--cache-test-set', default=False, action='store_true', help='Load test sets into memory')\n",
    "parser.add_argument('--cache-test-set-gpu', default=False, action='store_true', help='Load test sets into GPU memory')\n",
    "parser.add_argument('--test-batches', default=0, type=int, help='Number of minibatches to test on, or 0 for all of them')\n",
    "parser.add_argument('--eval-every', default=1, type=int, help='Evaluate on test set every N rounds')\n",
    "parser.add_argument('--device', default='0', type=device_list, help='Device to use for compute. Use \"cpu\" to force CPU. Otherwise, separate with commas to allow multi-GPU.')\n",
    "parser.add_argument('--no-eval', default=True, action='store_false', dest='eval')\n",
    "parser.add_argument('-o', '--outfile', default='output.log', type=argparse.FileType('a', encoding='ascii'))\n",
    "\n",
    "\n",
    "parser.add_argument('--model', type=str, choices=('VGG11_BN', 'VGG_SNIP', 'CNNNet'),\n",
    "                    default='VGG11_BN', help='Dataset to use')\n",
    "\n",
    "parser.add_argument('--prune_strategy', type=str, choices=('None', 'SNIP'),\n",
    "                    default='None', help='Dataset to use')\n",
    "parser.add_argument('--prune_at_first_round', default=False, action='store_true', dest='prune_at_first_round')\n",
    "parser.add_argument('--keep_ratio', type=float, default=0.0,\n",
    "                    help='local client batch size')    \n",
    "parser.add_argument('--prune_vote', type=int, default=1,\n",
    "                    help='local client batch size')\n",
    "\n",
    "parser.add_argument('--single_shot_pruning', default=False, action='store_true', dest='single_shot_pruning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70641184",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=['--dataset', 'cifar10', \n",
    "                               '--eta', '0.01', \n",
    "                               '--device', '3', \n",
    "                               '--distribution', 'classic_iid', \n",
    "                               '--total-clients', '10', \n",
    "                               '--clients', '10', \n",
    "                               '--batch-size', '64', \n",
    "                               '--rounds', '100', \n",
    "                               '--model', 'VGG11_BN', \n",
    "                               '--prune_strategy', 'SNIP',\n",
    "                               '--epochs', '2',\n",
    "                               '--keep_ratio', '0.9',\n",
    "                               '--prune_vote', '1',\n",
    "                               '--prune_at_first_round',\n",
    "                               '--single_shot_pruning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62671b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddbb79b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = fedsnip_obj.main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0fff754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# print('...')\n",
    "# name = input()\n",
    "# print('++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88461634",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching dataset...\n",
      "INFO:root:*********partition data***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download = True\n",
      "Files already downloaded and verified\n",
      "download = True\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:traindata_cls_counts = {0: {0: 503, 1: 483, 2: 487, 3: 527, 4: 473, 5: 548, 6: 491, 7: 502, 8: 482, 9: 504}, 1: {0: 501, 1: 503, 2: 529, 3: 503, 4: 494, 5: 468, 6: 462, 7: 497, 8: 507, 9: 536}, 2: {0: 510, 1: 540, 2: 498, 3: 461, 4: 516, 5: 483, 6: 502, 7: 515, 8: 492, 9: 483}, 3: {0: 487, 1: 487, 2: 493, 3: 498, 4: 523, 5: 482, 6: 500, 7: 487, 8: 495, 9: 548}, 4: {0: 505, 1: 480, 2: 488, 3: 488, 4: 480, 5: 509, 6: 537, 7: 490, 8: 490, 9: 533}, 5: {0: 504, 1: 480, 2: 511, 3: 494, 4: 530, 5: 509, 6: 521, 7: 489, 8: 490, 9: 472}, 6: {0: 512, 1: 509, 2: 512, 3: 488, 4: 488, 5: 465, 6: 490, 7: 538, 8: 506, 9: 492}, 7: {0: 472, 1: 521, 2: 463, 3: 549, 4: 502, 5: 515, 6: 518, 7: 496, 8: 479, 9: 485}, 8: {0: 505, 1: 485, 2: 487, 3: 505, 4: 497, 5: 512, 6: 508, 7: 501, 8: 531, 9: 469}, 9: {0: 501, 1: 512, 2: 532, 3: 487, 4: 497, 5: 509, 6: 471, 7: 485, 8: 528, 9: 478}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000******************\n",
      "download = True\n",
      "Files already downloaded and verified\n",
      "download = True\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:train_dl_global number = 782\n",
      "INFO:root:test_dl_global number = 157\n",
      "INFO:root:client_idx = 0, local_sample_number = 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download = True\n",
      "Files already downloaded and verified\n",
      "download = True\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 0, batch_num_train_local = 79, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 1, local_sample_number = 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download = True\n",
      "Files already downloaded and verified\n",
      "download = True\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 1, batch_num_train_local = 79, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 2, local_sample_number = 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download = True\n",
      "Files already downloaded and verified\n",
      "download = True\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 2, batch_num_train_local = 79, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 3, local_sample_number = 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download = True\n",
      "Files already downloaded and verified\n",
      "download = True\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 3, batch_num_train_local = 79, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 4, local_sample_number = 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download = True\n",
      "Files already downloaded and verified\n",
      "download = True\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 4, batch_num_train_local = 79, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 5, local_sample_number = 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download = True\n",
      "Files already downloaded and verified\n",
      "download = True\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 5, batch_num_train_local = 79, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 6, local_sample_number = 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download = True\n",
      "Files already downloaded and verified\n",
      "download = True\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 6, batch_num_train_local = 79, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 7, local_sample_number = 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download = True\n",
      "Files already downloaded and verified\n",
      "download = True\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 7, batch_num_train_local = 79, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 8, local_sample_number = 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download = True\n",
      "Files already downloaded and verified\n",
      "download = True\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 8, batch_num_train_local = 79, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 9, local_sample_number = 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download = True\n",
      "Files already downloaded and verified\n",
      "download = True\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 9, batch_num_train_local = 79, batch_num_test_local = 16\n",
      "Initializing clients...\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mslimfun\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/slimfun/feddst/runs/2olzjmrz\" target=\"_blank\">FedDST(d)</a></strong> to <a href=\"https://wandb.ai/slimfun/feddst\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is me!!!\n",
      "It is me!!!\n",
      "It is me!!!\n",
      "It is me!!!\n",
      "It is me!!!\n",
      "It is me!!!\n",
      "It is me!!!\n",
      "It is me!!!\n",
      "It is me!!!\n",
      "It is me!!!\n",
      "It is me!!!\n",
      "client: 6 **************\n",
      "all params num: 9747136; num_params_to_keep: 974713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(974713, device='cuda:3')\n",
      "client: 9 **************\n",
      "all params num: 9747136; num_params_to_keep: 974713\n",
      "tensor(974713, device='cuda:3')\n",
      "client: 5 **************\n",
      "all params num: 9747136; num_params_to_keep: 974713\n",
      "tensor(974714, device='cuda:3')\n",
      "client: 8 **************\n",
      "all params num: 9747136; num_params_to_keep: 974713\n",
      "tensor(974713, device='cuda:3')\n",
      "client: 2 **************\n",
      "all params num: 9747136; num_params_to_keep: 974713\n",
      "tensor(974714, device='cuda:3')\n",
      "client: 7 **************\n",
      "all params num: 9747136; num_params_to_keep: 974713\n",
      "tensor(974713, device='cuda:3')\n",
      "client: 3 **************\n",
      "all params num: 9747136; num_params_to_keep: 974713\n",
      "tensor(974713, device='cuda:3')\n",
      "client: 0 **************\n",
      "all params num: 9747136; num_params_to_keep: 974713\n",
      "tensor(974713, device='cuda:3')\n",
      "client: 1 **************\n",
      "all params num: 9747136; num_params_to_keep: 974713\n",
      "tensor(974713, device='cuda:3')\n",
      "client: 4 **************\n",
      "all params num: 9747136; num_params_to_keep: 974713\n",
      "tensor(974714, device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "# while True:\n",
    "#     debug_info = next(run)\n",
    "#     print(debug_info.msg)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     input()\n",
    "debug_info = next(run)\n",
    "(model_list, server) = debug_info.obj\n",
    "# global_model = debug_info.obj\n",
    "# aggregated_masks = debug_info.obj[0]\n",
    "# cl_mask_prarms = debug_info.obj[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6034449f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[10., 10., 10.],\n",
      "          [ 7.,  9.,  9.],\n",
      "          [ 9., 10., 10.]],\n",
      "\n",
      "         [[10., 10., 10.],\n",
      "          [ 9.,  9.,  9.],\n",
      "          [10., 10., 10.]],\n",
      "\n",
      "         [[10., 10.,  9.],\n",
      "          [10., 10., 10.],\n",
      "          [ 9.,  0., 10.]]],\n",
      "\n",
      "\n",
      "        [[[10.,  9., 10.],\n",
      "          [10., 10., 10.],\n",
      "          [10., 10., 10.]],\n",
      "\n",
      "         [[10.,  9., 10.],\n",
      "          [10.,  5., 10.],\n",
      "          [10., 10., 10.]],\n",
      "\n",
      "         [[10., 10., 10.],\n",
      "          [ 9., 10., 10.],\n",
      "          [ 9., 10.,  9.]]],\n",
      "\n",
      "\n",
      "        [[[ 6., 10., 10.],\n",
      "          [10.,  9., 10.],\n",
      "          [10.,  8., 10.]],\n",
      "\n",
      "         [[10.,  1., 10.],\n",
      "          [10., 10., 10.],\n",
      "          [ 7., 10., 10.]],\n",
      "\n",
      "         [[10., 10., 10.],\n",
      "          [10.,  9.,  5.],\n",
      "          [10., 10.,  9.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 7., 10.,  8.],\n",
      "          [ 7.,  5.,  9.],\n",
      "          [ 8.,  8., 10.]],\n",
      "\n",
      "         [[10.,  5., 10.],\n",
      "          [ 8., 10., 10.],\n",
      "          [10., 10.,  9.]],\n",
      "\n",
      "         [[10.,  9., 10.],\n",
      "          [ 0.,  8.,  9.],\n",
      "          [ 9.,  8.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[10., 10., 10.],\n",
      "          [ 9.,  8.,  8.],\n",
      "          [10., 10., 10.]],\n",
      "\n",
      "         [[ 9.,  9., 10.],\n",
      "          [ 2., 10., 10.],\n",
      "          [10., 10., 10.]],\n",
      "\n",
      "         [[ 9., 10., 10.],\n",
      "          [ 7.,  8.,  5.],\n",
      "          [10.,  9.,  9.]]],\n",
      "\n",
      "\n",
      "        [[[ 9., 10., 10.],\n",
      "          [ 9.,  0., 10.],\n",
      "          [ 9., 10.,  9.]],\n",
      "\n",
      "         [[10., 10., 10.],\n",
      "          [ 9., 10., 10.],\n",
      "          [10.,  9.,  9.]],\n",
      "\n",
      "         [[10.,  9., 10.],\n",
      "          [10., 10., 10.],\n",
      "          [ 9., 10., 10.]]]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(f'applyed_masks.txt', 'r') as f:\n",
    "    masks_str = f.readline()\n",
    "#     print(masks_str)\n",
    "    masks = json.loads(masks_str)\n",
    "    for i in range(len(masks)):\n",
    "        masks[i] = torch.from_numpy(np.asarray(masks[i]))\n",
    "        \n",
    "print(masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in model_list[0][1]:\n",
    "    print(name)\n",
    "    print(model_list[0][1][name].dtype)\n",
    "#     print(model_list[0][1][name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba2be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_c = 0.0\n",
    "total = 0.0\n",
    "\n",
    "for name, param in server.model.state_dict().items():\n",
    "    a = param.view(-1).to(device='cpu', copy=True).numpy()\n",
    "    pruned_c +=sum(np.where(a, 0, 1))\n",
    "    total += param.numel()\n",
    "print(f'global model zero params: {pruned_c / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8968b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in server.model.state_dict():\n",
    "#     print(name)\n",
    "server.model = server.model.to('cuda:3')\n",
    "params = server.model.cpu().state_dict()\n",
    "print(params['features.0.weight'])\n",
    "\n",
    "print('***********'*3)\n",
    "\n",
    "# params['features.0.weight'] = torch.zeros_like(params['features.0.weight'])\n",
    "params['features.0.weight'][0][0][0] = 2.\n",
    "\n",
    "print(server.model.state_dict()['features.0.weight'])\n",
    "\n",
    "\n",
    "print(params['features.0.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d24b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_c = 0.0\n",
    "total = 0.0\n",
    "for name, param in global_model.state_dict().items():\n",
    "    a = param.view(-1).to(device='cpu', copy=True).numpy()\n",
    "    pruned_c +=sum(np.where(a, 0, 1))\n",
    "    total += param.numel()\n",
    "print(f'global model zero params: {pruned_c / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27829812",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_c = 0.\n",
    "total = 0.\n",
    "for name,mask in global_model.mask.items():\n",
    "#     print(mask)\n",
    "    prune_c += sum(np.where(mask.to('cpu', copy=True).view(-1).numpy(), 0, 1))\n",
    "    total += mask.numel()\n",
    "    \n",
    "print(prune_c / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d7b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb36d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "pruned_c = 0.0\n",
    "total = 0.0\n",
    "for name, param in global_model.state_dict().items():\n",
    "    a = param.view(-1).to(device='cpu', copy=True).numpy()\n",
    "    pruned_c +=sum(np.where(a, 0, 1))\n",
    "    total += param.numel()\n",
    "print(f'global model zero params: {pruned_c / total}')\n",
    "\n",
    "gcp_model = copy.deepcopy(global_model)\n",
    "\n",
    "\n",
    "\n",
    "prune_c = 0.\n",
    "total = 0.\n",
    "for name, params in gcp_model.state_dict().items():\n",
    "#     print(name)\n",
    "    prune_c += sum(np.where(params.to('cpu', copy=True).view(-1).numpy(), 0, 1))\n",
    "    total += params.numel()\n",
    "    \n",
    "print(prune_c / total)\n",
    "    \n",
    "# print('*'*10)\n",
    "# for name, mask in global_model.mask.items():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bca0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in aggregated_masks:\n",
    "#     print(name)\n",
    "#     print(aggregated_masks[name].dtype)\n",
    "\n",
    "def count_mask(state_dict):\n",
    "    non_zero = 0.\n",
    "    total = 0.\n",
    "    for name in state_dict:\n",
    "        non_zero += torch.count_nonzero(state_dict[name])\n",
    "        total += state_dict[name].numel()\n",
    "    return 1 - non_zero / total\n",
    "# a = aggregated_masks['features.4.weight']\n",
    "print(a.shape)\n",
    "# print(torch.count_nonzero(a))\n",
    "print(count_mask(aggregated_masks))\n",
    "mask = torch.where(a>=1, 1, 0)\n",
    "# print(torch.count_nonzero(mask))\n",
    "print(count_mask(cl_mask_prarms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "keep_masks = {0: [torch.Tensor([0, 1, 0, 1, 1])], 1: [torch.Tensor([1, 1, 0, 0, 1])]}\n",
    "\n",
    "model_list = [[2, torch.Tensor([1,2,3,4,5])], [3, torch.Tensor([1,2,3,4,5])]]\n",
    "\n",
    "masks = copy.deepcopy(keep_masks)\n",
    "for c, m in masks.items():\n",
    "    for i in range(len(m)):\n",
    "        m[i] *= model_list[c][0]\n",
    "    \n",
    "print(masks)\n",
    "\n",
    "total_masks = copy.deepcopy(masks)\n",
    "for c,m in total_masks.items():\n",
    "    if c!= 0:\n",
    "        for i in range(len(m)):\n",
    "            total_masks[0][i] += m[i]\n",
    "        \n",
    "print(total_masks)\n",
    "\n",
    "for c, m in masks.items():\n",
    "    for i in range(len(m)):\n",
    "        m[i] /= total_masks[0][i]\n",
    "        masks[c][i] = torch.where(torch.isnan(m[i]), torch.full_like(m[i], 0), m[i])\n",
    "#     print(torch.isnan(m))\n",
    "print(masks)\n",
    "#     m /= sum(model_list[i][0] for i in range(len(model_list)))\n",
    "\n",
    "# print(masks[0] * model_list[0][1])\n",
    "model_list[0][1] *= masks[0]\n",
    "# print(model_list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb781364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "list_tensor = [torch.Tensor([1,2]), torch.Tensor([2,3])]\n",
    "t = torch.stack(list_tensor)\n",
    "list_tensor *= 2\n",
    "list_tensor\n",
    "# print(t)\n",
    "# a = torch.from_numpy(np.asarray(list_tensor))\n",
    "# print(a)\n",
    "# torch.cat(list_tensor, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fc21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([2,3]).type(torch.float64)\n",
    "b = torch.Tensor([1,2]).type(torch.float64)\n",
    "print(a.dtype == b.dtype)\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8e4a56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0406,  0.3143, -0.1377,  0.1393, -0.3008,  0.2192],\n",
      "        [ 0.0291,  0.2250, -0.0986,  0.0997, -0.2153,  0.1569]])\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "before step:  [Parameter containing:\n",
      "tensor([[-0.2029, -0.3128, -0.3821, -0.3446, -0.0828,  0.2239],\n",
      "        [ 0.2207, -0.3937,  0.2547, -0.3195, -0.0863, -0.1655]],\n",
      "       requires_grad=True)]\n",
      "after step:  [Parameter containing:\n",
      "tensor([[-0.2027, -0.3125, -0.3817, -0.3442, -0.0827,  0.2237],\n",
      "        [ 0.2205, -0.3933,  0.2544, -0.3191, -0.0862, -0.1654]],\n",
      "       requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(6, 2, bias=False),\n",
    "          nn.Sigmoid(),\n",
    "        )\n",
    "input = torch.randn(6)\n",
    "target = torch.randn(2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.1)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "output = model(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "print(list(model.parameters())[0].grad)\n",
    "optimizer.zero_grad() \n",
    "print(list(model.parameters())[0].grad)\n",
    "\n",
    "\n",
    "print(\"before step: \", list(model.parameters()))\n",
    "optimizer.step()\n",
    "print(\"after step: \", list(model.parameters()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
