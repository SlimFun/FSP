{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc456178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from scipy import test\n",
    "import torch\n",
    "import torch.cuda\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import argparse\n",
    "import gc\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "import wandb\n",
    "\n",
    "from datasets import get_dataset\n",
    "from models.models import all_models\n",
    "\n",
    "from client import Client\n",
    "from utils import *\n",
    "\n",
    "import fedsnip as fedsnip_obj\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "def device_list(x):\n",
    "    if x == 'cpu':\n",
    "        return [x]\n",
    "    return [int(y) for y in x.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2442c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--target_keep_ratio'], dest='target_keep_ratio', nargs=None, const=None, default=0.1, type=<class 'float'>, choices=None, help='server target keep ratio', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--eta', type=float, help='learning rate', default=0.01)\n",
    "parser.add_argument('--clients', type=int, help='number of clients per round', default=20)\n",
    "parser.add_argument('--rounds', type=int, help='number of global rounds', default=400)\n",
    "parser.add_argument('--epochs', type=int, help='number of local epochs', default=10)\n",
    "parser.add_argument('--dataset', type=str, choices=('mnist', 'emnist', 'cifar10', 'cifar100'),\n",
    "                    default='mnist', help='Dataset to use')\n",
    "parser.add_argument('--distribution', type=str, choices=('dirichlet', 'lotteryfl', 'iid', 'classic_iid'), default='dirichlet',\n",
    "                    help='how should the dataset be distributed?')\n",
    "parser.add_argument('--beta', type=float, default=0.1, help='Beta parameter (unbalance rate) for Dirichlet distribution')\n",
    "parser.add_argument('--total-clients', type=int, help='split the dataset between this many clients. Ignored for EMNIST.', default=400)\n",
    "parser.add_argument('--min-samples', type=int, default=0, help='minimum number of samples required to allow a client to participate')\n",
    "parser.add_argument('--samples-per-client', type=int, default=20, help='samples to allocate to each client (per class, for lotteryfl, or per client, for iid)')\n",
    "parser.add_argument('--prox', type=float, default=0, help='coefficient to proximal term (i.e. in FedProx)')\n",
    "\n",
    "parser.add_argument('--batch-size', type=int, default=32,\n",
    "                    help='local client batch size')\n",
    "parser.add_argument('--l2', default=1e-5, type=float, help='L2 regularization strength')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='Local client SGD momentum parameter')\n",
    "parser.add_argument('--cache-test-set', default=False, action='store_true', help='Load test sets into memory')\n",
    "parser.add_argument('--cache-test-set-gpu', default=False, action='store_true', help='Load test sets into GPU memory')\n",
    "parser.add_argument('--test-batches', default=0, type=int, help='Number of minibatches to test on, or 0 for all of them')\n",
    "parser.add_argument('--eval-every', default=1, type=int, help='Evaluate on test set every N rounds')\n",
    "parser.add_argument('--device', default='0', type=device_list, help='Device to use for compute. Use \"cpu\" to force CPU. Otherwise, separate with commas to allow multi-GPU.')\n",
    "parser.add_argument('--no-eval', default=True, action='store_false', dest='eval')\n",
    "parser.add_argument('-o', '--outfile', default='output.log', type=argparse.FileType('a', encoding='ascii'))\n",
    "\n",
    "parser.add_argument('--clip_grad', default=False, action='store_true', dest='clip_grad')\n",
    "\n",
    "parser.add_argument('--model', type=str, choices=('VGG11_BN', 'VGG_SNIP', 'CNNNet', 'CIFAR10Net'),\n",
    "                    default='VGG11_BN', help='Dataset to use')\n",
    "\n",
    "parser.add_argument('--prune_strategy', type=str, choices=('None', 'SNIP', 'SNAP', 'random_masks'),\n",
    "                    default='None', help='Dataset to use')\n",
    "parser.add_argument('--prune_at_first_round', default=False, action='store_true', dest='prune_at_first_round')\n",
    "parser.add_argument('--keep_ratio', type=float, default=0.0,\n",
    "                    help='local client batch size')         \n",
    "parser.add_argument('--prune_vote', type=int, default=1,\n",
    "                    help='local client batch size')\n",
    "\n",
    "parser.add_argument('--single_shot_pruning', default=False, action='store_true', dest='single_shot_pruning')\n",
    "\n",
    "parser.add_argument('--partition_method', type=str, default='homo', metavar='N',\n",
    "                        help='how to partition the dataset on local workers')\n",
    "\n",
    "parser.add_argument('--partition_alpha', type=float, default=0.5, metavar='PA',\n",
    "                    help='partition alpha (default: 0.5)')\n",
    "\n",
    "parser.add_argument('--target_keep_ratio', default=0.1, type=float, help='server target keep ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70641184",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=['--dataset', 'cifar10', \n",
    "                               '--eta', '0.01', \n",
    "                               '--device', '3', \n",
    "                               '--distribution', 'classic_iid', \n",
    "                               '--total-clients', '10', \n",
    "                               '--clients', '10', \n",
    "                               '--batch-size', '64', \n",
    "                               '--rounds', '100', \n",
    "                               '--model', 'CNNNet', \n",
    "                               '--prune_strategy', 'SNIP',\n",
    "                               '--epochs', '2',\n",
    "                               '--keep_ratio', '0.1',\n",
    "                               '--prune_vote', '1',\n",
    "                               '--prune_at_first_round',\n",
    "                               '--single_shot_pruning',\n",
    "                               '--partition_method', 'hetero',\n",
    "                               '--partition_alpha', '0.5',\n",
    "                               '--target_keep_ratio', '0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62671b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddbb79b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = fedsnip_obj.main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fff754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# print('...')\n",
    "# name = input()\n",
    "# print('++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88461634",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching dataset...\n",
      "INFO:root:*********partition data***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:N = 50000\n",
      "INFO:root:traindata_cls_counts = {0: {0: 953, 1: 142, 2: 141, 3: 75, 4: 695, 5: 819, 7: 2482}, 1: {0: 16, 1: 43, 2: 902, 3: 1650, 4: 86, 5: 182, 7: 693, 8: 110, 9: 6}, 2: {0: 9, 1: 8, 2: 290, 3: 769, 4: 841, 5: 283, 6: 119, 7: 1044, 8: 1014, 9: 58}, 3: {0: 395, 1: 1200, 2: 48, 3: 68, 4: 896, 5: 681, 6: 90, 7: 17, 8: 1351, 9: 301}, 4: {0: 504, 1: 2917, 2: 570, 3: 721, 4: 121, 5: 356}, 5: {0: 1262, 1: 71, 2: 325, 3: 119, 4: 1560, 5: 14, 6: 1, 7: 85, 8: 366}, 6: {0: 9, 1: 273, 2: 1657, 3: 40, 4: 1, 5: 130, 6: 1911, 7: 21, 8: 1160}, 7: {0: 722, 1: 3, 2: 281, 3: 738, 4: 22, 5: 974, 6: 624, 7: 1, 8: 82, 9: 878}, 8: {0: 1127, 1: 153, 2: 680, 3: 500, 4: 698, 5: 1139, 6: 92, 7: 23, 8: 1, 9: 3665}, 9: {0: 3, 1: 190, 2: 106, 3: 320, 4: 80, 5: 422, 6: 2163, 7: 634, 8: 916, 9: 92}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000******************\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:train_dl_global number = 782\n",
      "INFO:root:test_dl_global number = 157\n",
      "INFO:root:client_idx = 0, local_sample_number = 5307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 0, batch_num_train_local = 83, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 1, local_sample_number = 3688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 1, batch_num_train_local = 58, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 2, local_sample_number = 4435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 2, batch_num_train_local = 70, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 3, local_sample_number = 5047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 3, batch_num_train_local = 79, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 4, local_sample_number = 5189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 4, batch_num_train_local = 82, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 5, local_sample_number = 3803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 5, batch_num_train_local = 60, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 6, local_sample_number = 5202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 6, batch_num_train_local = 82, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 7, local_sample_number = 4325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 7, batch_num_train_local = 68, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 8, local_sample_number = 8078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 8, batch_num_train_local = 127, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 9, local_sample_number = 4926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 9, batch_num_train_local = 77, batch_num_test_local = 16\n",
      "Initializing clients...\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mslimfun\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/slimfun/fedsnip/runs/exdnkxba\" target=\"_blank\">FedAVG(d)SNIP-target_kr0.1-0.1-classic_iid-prune_at_1rTrue-single_shot_pruningTrue-lr0.01-clip_gradFalse</a></strong> to <a href=\"https://wandb.ai/slimfun/fedsnip\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server model param size: 69007680\n",
      "client: 6 **************\n",
      "No post init specified in SNIP\n",
      "num_params_to_keep: 215587, percentage: 0.09999999999999998\n",
      "pruning 510 percentage 0.2125 length_nonzero 2400.0\n",
      "pruning 26132 percentage 0.510390625 length_nonzero 51200.0\n",
      "pruning 1911811 percentage 0.9116225242614746 length_nonzero 2097152.0\n",
      "pruning 1833 percentage 0.3580078125 length_nonzero 5120.0\n",
      "final percentage after snip: 0.9000005566193169\n",
      "client: 3 **************\n",
      "No post init specified in SNIP\n",
      "num_params_to_keep: 215587, percentage: 0.09999999999999998\n",
      "pruning 384 percentage 0.16 length_nonzero 2400.0\n",
      "pruning 25089 percentage 0.49001953125 length_nonzero 51200.0\n",
      "pruning 1913021 percentage 0.9121994972229004 length_nonzero 2097152.0\n",
      "pruning 1792 percentage 0.35 length_nonzero 5120.0\n",
      "final percentage after snip: 0.9000005566193169\n",
      "client: 7 **************\n",
      "No post init specified in SNIP\n",
      "num_params_to_keep: 215587, percentage: 0.09999999999999998\n",
      "pruning 451 percentage 0.18791666666666668 length_nonzero 2400.0\n",
      "pruning 23296 percentage 0.455 length_nonzero 51200.0\n",
      "pruning 1914792 percentage 0.9130439758300781 length_nonzero 2097152.0\n",
      "pruning 1747 percentage 0.3412109375 length_nonzero 5120.0\n",
      "final percentage after snip: 0.9000005566193169\n",
      "client: 8 **************\n",
      "No post init specified in SNIP\n",
      "num_params_to_keep: 215587, percentage: 0.09999999999999998\n",
      "pruning 429 percentage 0.17875 length_nonzero 2400.0\n",
      "pruning 25684 percentage 0.501640625 length_nonzero 51200.0\n",
      "pruning 1911947 percentage 0.9116873741149902 length_nonzero 2097152.0\n",
      "pruning 2226 percentage 0.434765625 length_nonzero 5120.0\n",
      "final percentage after snip: 0.9000005566193169\n",
      "client: 2 **************\n",
      "No post init specified in SNIP\n",
      "num_params_to_keep: 215587, percentage: 0.09999999999999998\n",
      "pruning 398 percentage 0.16583333333333333 length_nonzero 2400.0\n",
      "pruning 24863 percentage 0.48560546875 length_nonzero 51200.0\n",
      "pruning 1913264 percentage 0.9123153686523438 length_nonzero 2097152.0\n",
      "pruning 1761 percentage 0.3439453125 length_nonzero 5120.0\n",
      "final percentage after snip: 0.9000005566193169\n",
      "client: 0 **************\n",
      "No post init specified in SNIP\n",
      "num_params_to_keep: 215587, percentage: 0.09999999999999998\n",
      "pruning 389 percentage 0.16208333333333333 length_nonzero 2400.0\n",
      "pruning 24587 percentage 0.48021484375 length_nonzero 51200.0\n",
      "pruning 1912870 percentage 0.9121274948120117 length_nonzero 2097152.0\n",
      "pruning 2440 percentage 0.4765625 length_nonzero 5120.0\n",
      "final percentage after snip: 0.9000005566193169\n",
      "client: 4 **************\n",
      "No post init specified in SNIP\n",
      "num_params_to_keep: 215587, percentage: 0.09999999999999998\n",
      "pruning 484 percentage 0.20166666666666666 length_nonzero 2400.0\n",
      "pruning 26107 percentage 0.50990234375 length_nonzero 51200.0\n",
      "pruning 1911593 percentage 0.9115185737609863 length_nonzero 2097152.0\n",
      "pruning 2102 percentage 0.410546875 length_nonzero 5120.0\n",
      "final percentage after snip: 0.9000005566193169\n",
      "client: 9 **************\n",
      "No post init specified in SNIP\n",
      "num_params_to_keep: 215587, percentage: 0.09999999999999998\n",
      "pruning 394 percentage 0.16416666666666666 length_nonzero 2400.0\n",
      "pruning 24738 percentage 0.4831640625 length_nonzero 51200.0\n",
      "pruning 1913267 percentage 0.9123167991638184 length_nonzero 2097152.0\n",
      "pruning 1887 percentage 0.3685546875 length_nonzero 5120.0\n",
      "final percentage after snip: 0.9000005566193169\n",
      "client: 5 **************\n",
      "No post init specified in SNIP\n",
      "num_params_to_keep: 215587, percentage: 0.09999999999999998\n",
      "pruning 436 percentage 0.18166666666666667 length_nonzero 2400.0\n",
      "pruning 25020 percentage 0.488671875 length_nonzero 51200.0\n",
      "pruning 1912835 percentage 0.9121108055114746 length_nonzero 2097152.0\n",
      "pruning 1995 percentage 0.3896484375 length_nonzero 5120.0\n",
      "final percentage after snip: 0.9000005566193169\n",
      "client: 1 **************\n",
      "No post init specified in SNIP\n",
      "num_params_to_keep: 215587, percentage: 0.09999999999999998\n",
      "pruning 419 percentage 0.17458333333333334 length_nonzero 2400.0\n",
      "pruning 25553 percentage 0.49908203125 length_nonzero 51200.0\n",
      "pruning 1912457 percentage 0.9119305610656738 length_nonzero 2097152.0\n",
      "pruning 1857 percentage 0.3626953125 length_nonzero 5120.0\n",
      "final percentage after snip: 0.9000005566193169\n",
      "merge local masks\n",
      "server masked 89.97421736247327% params\n"
     ]
    }
   ],
   "source": [
    "# while True:\n",
    "#     debug_info = next(run)\n",
    "#     print(debug_info.msg)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     input()\n",
    "debug_info = next(run)\n",
    "masks = debug_info.obj\n",
    "# global_model = debug_info.obj\n",
    "# aggregated_masks = debug_info.obj[0]\n",
    "# cl_mask_prarms = debug_info.obj[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f81c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = server.masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = masks\n",
    "def count_vote(masks, vote):\n",
    "    tc = 0.\n",
    "    keeped = 0.\n",
    "    for i in range(len(masks)):\n",
    "        tc += masks[i].numel()\n",
    "        m = masks[i].clone().detach()\n",
    "        keeped += torch.sum(torch.where(m >= vote, 1, 0))\n",
    "\n",
    "    print(\"keeped: {}, total: {}, pct: {}\".format(keeped, tc, keeped/tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25343c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('vote {}'.format(i))\n",
    "    count_vote(server.masks, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = server.masks\n",
    "for i in range(len(masks)):\n",
    "    print(masks[i].shape)\n",
    "flat_masks = torch.cat([m.flatten() for m in masks])\n",
    "keep_num = len(flat_masks) * 0.1\n",
    "threshold, indices = flat_masks.topk(int(keep_num))\n",
    "global_masks = torch.zeros_like(flat_masks)\n",
    "global_masks[indices] = 1\n",
    "print(indices.sort())\n",
    "# len(indices)\n",
    "print(global_masks[:25])\n",
    "print(global_masks.sum())\n",
    "\n",
    "idx = 0\n",
    "ms = []\n",
    "for i in range(len(masks)):\n",
    "    m = global_masks[idx:idx+masks[i].numel()].reshape(masks[i].size())\n",
    "    idx += masks[i].numel()\n",
    "    print(m.shape)\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a143b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5])\n",
    "c = torch.tensor([6,7,8,9])\n",
    "ts = torch.cat([e.flatten() for e in [a,b,c]])\n",
    "print(ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba2be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_c = 0.0\n",
    "total = 0.0\n",
    "\n",
    "for name, param in server.model.state_dict().items():\n",
    "    a = param.view(-1).to(device='cpu', copy=True).numpy()\n",
    "    pruned_c +=sum(np.where(a, 0, 1))\n",
    "    total += param.numel()\n",
    "print(f'global model zero params: {pruned_c / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8968b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in server.model.state_dict():\n",
    "#     print(name)\n",
    "server.model = server.model.to('cuda:3')\n",
    "params = server.model.cpu().state_dict()\n",
    "print(params['features.0.weight'])\n",
    "\n",
    "print('***********'*3)\n",
    "\n",
    "# params['features.0.weight'] = torch.zeros_like(params['features.0.weight'])\n",
    "params['features.0.weight'][0][0][0] = 2.\n",
    "\n",
    "print(server.model.state_dict()['features.0.weight'])\n",
    "\n",
    "\n",
    "print(params['features.0.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d24b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_c = 0.0\n",
    "total = 0.0\n",
    "for name, param in global_model.state_dict().items():\n",
    "    a = param.view(-1).to(device='cpu', copy=True).numpy()\n",
    "    pruned_c +=sum(np.where(a, 0, 1))\n",
    "    total += param.numel()\n",
    "print(f'global model zero params: {pruned_c / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27829812",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_c = 0.\n",
    "total = 0.\n",
    "for name,mask in global_model.mask.items():\n",
    "#     print(mask)\n",
    "    prune_c += sum(np.where(mask.to('cpu', copy=True).view(-1).numpy(), 0, 1))\n",
    "    total += mask.numel()\n",
    "    \n",
    "print(prune_c / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d7b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb36d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "pruned_c = 0.0\n",
    "total = 0.0\n",
    "for name, param in global_model.state_dict().items():\n",
    "    a = param.view(-1).to(device='cpu', copy=True).numpy()\n",
    "    pruned_c +=sum(np.where(a, 0, 1))\n",
    "    total += param.numel()\n",
    "print(f'global model zero params: {pruned_c / total}')\n",
    "\n",
    "gcp_model = copy.deepcopy(global_model)\n",
    "\n",
    "\n",
    "\n",
    "prune_c = 0.\n",
    "total = 0.\n",
    "for name, params in gcp_model.state_dict().items():\n",
    "#     print(name)\n",
    "    prune_c += sum(np.where(params.to('cpu', copy=True).view(-1).numpy(), 0, 1))\n",
    "    total += params.numel()\n",
    "    \n",
    "print(prune_c / total)\n",
    "    \n",
    "# print('*'*10)\n",
    "# for name, mask in global_model.mask.items():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bca0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in aggregated_masks:\n",
    "#     print(name)\n",
    "#     print(aggregated_masks[name].dtype)\n",
    "\n",
    "def count_mask(state_dict):\n",
    "    non_zero = 0.\n",
    "    total = 0.\n",
    "    for name in state_dict:\n",
    "        non_zero += torch.count_nonzero(state_dict[name])\n",
    "        total += state_dict[name].numel()\n",
    "    return 1 - non_zero / total\n",
    "# a = aggregated_masks['features.4.weight']\n",
    "print(a.shape)\n",
    "# print(torch.count_nonzero(a))\n",
    "print(count_mask(aggregated_masks))\n",
    "mask = torch.where(a>=1, 1, 0)\n",
    "# print(torch.count_nonzero(mask))\n",
    "print(count_mask(cl_mask_prarms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "keep_masks = {0: [torch.Tensor([0, 1, 0, 1, 1])], 1: [torch.Tensor([1, 1, 0, 0, 1])]}\n",
    "\n",
    "model_list = [[2, torch.Tensor([1,2,3,4,5])], [3, torch.Tensor([1,2,3,4,5])]]\n",
    "\n",
    "masks = copy.deepcopy(keep_masks)\n",
    "for c, m in masks.items():\n",
    "    for i in range(len(m)):\n",
    "        m[i] *= model_list[c][0]\n",
    "    \n",
    "print(masks)\n",
    "\n",
    "total_masks = copy.deepcopy(masks)\n",
    "for c,m in total_masks.items():\n",
    "    if c!= 0:\n",
    "        for i in range(len(m)):\n",
    "            total_masks[0][i] += m[i]\n",
    "        \n",
    "print(total_masks)\n",
    "\n",
    "for c, m in masks.items():\n",
    "    for i in range(len(m)):\n",
    "        m[i] /= total_masks[0][i]\n",
    "        masks[c][i] = torch.where(torch.isnan(m[i]), torch.full_like(m[i], 0), m[i])\n",
    "#     print(torch.isnan(m))\n",
    "print(masks)\n",
    "#     m /= sum(model_list[i][0] for i in range(len(model_list)))\n",
    "\n",
    "# print(masks[0] * model_list[0][1])\n",
    "model_list[0][1] *= masks[0]\n",
    "# print(model_list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb781364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "list_tensor = [torch.Tensor([1,2]), torch.Tensor([2,3])]\n",
    "t = torch.stack(list_tensor)\n",
    "list_tensor *= 2\n",
    "list_tensor\n",
    "# print(t)\n",
    "# a = torch.from_numpy(np.asarray(list_tensor))\n",
    "# print(a)\n",
    "# torch.cat(list_tensor, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fc21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([2,3]).type(torch.float64)\n",
    "b = torch.Tensor([1,2]).type(torch.float64)\n",
    "print(a.dtype == b.dtype)\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(6, 2, bias=False),\n",
    "          nn.Sigmoid(),\n",
    "        )\n",
    "input = torch.randn(6)\n",
    "target = torch.randn(2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.1)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "output = model(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "print(list(model.parameters())[0].grad)\n",
    "optimizer.zero_grad() \n",
    "print(list(model.parameters())[0].grad)\n",
    "\n",
    "\n",
    "print(\"before step: \", list(model.parameters()))\n",
    "optimizer.step()\n",
    "print(\"after step: \", list(model.parameters()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
