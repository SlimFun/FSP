{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc456178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from scipy import test\n",
    "import torch\n",
    "import torch.cuda\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import argparse\n",
    "import gc\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "import wandb\n",
    "\n",
    "from datasets import get_dataset\n",
    "from models.models import all_models\n",
    "\n",
    "from client import Client\n",
    "from utils import *\n",
    "\n",
    "import fedsnip as fedsnip_obj\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "def device_list(x):\n",
    "    if x == 'cpu':\n",
    "        return [x]\n",
    "    return [int(y) for y in x.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2442c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--saliency_mode'], dest='saliency_mode', nargs=None, const=None, default=None, type=<class 'str'>, choices=('saliency', 'mask'), help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--eta', type=float, help='learning rate', default=0.01)\n",
    "parser.add_argument('--clients', type=int, help='number of clients per round', default=20)\n",
    "parser.add_argument('--rounds', type=int, help='number of global rounds', default=400)\n",
    "parser.add_argument('--epochs', type=int, help='number of local epochs', default=10)\n",
    "parser.add_argument('--dataset', type=str, choices=('mnist', 'emnist', 'cifar10', 'cifar100'),\n",
    "                    default='mnist', help='Dataset to use')\n",
    "parser.add_argument('--distribution', type=str, choices=('dirichlet', 'lotteryfl', 'iid', 'classic_iid'), default='dirichlet',\n",
    "                    help='how should the dataset be distributed?')\n",
    "parser.add_argument('--beta', type=float, default=0.1, help='Beta parameter (unbalance rate) for Dirichlet distribution')\n",
    "parser.add_argument('--total-clients', type=int, help='split the dataset between this many clients. Ignored for EMNIST.', default=400)\n",
    "parser.add_argument('--min-samples', type=int, default=0, help='minimum number of samples required to allow a client to participate')\n",
    "parser.add_argument('--samples-per-client', type=int, default=20, help='samples to allocate to each client (per class, for lotteryfl, or per client, for iid)')\n",
    "parser.add_argument('--prox', type=float, default=0, help='coefficient to proximal term (i.e. in FedProx)')\n",
    "\n",
    "parser.add_argument('--batch-size', type=int, default=32,\n",
    "                    help='local client batch size')\n",
    "parser.add_argument('--l2', default=1e-5, type=float, help='L2 regularization strength')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='Local client SGD momentum parameter')\n",
    "parser.add_argument('--cache-test-set', default=False, action='store_true', help='Load test sets into memory')\n",
    "parser.add_argument('--cache-test-set-gpu', default=False, action='store_true', help='Load test sets into GPU memory')\n",
    "parser.add_argument('--test-batches', default=0, type=int, help='Number of minibatches to test on, or 0 for all of them')\n",
    "parser.add_argument('--eval-every', default=1, type=int, help='Evaluate on test set every N rounds')\n",
    "parser.add_argument('--device', default='0', type=device_list, help='Device to use for compute. Use \"cpu\" to force CPU. Otherwise, separate with commas to allow multi-GPU.')\n",
    "parser.add_argument('--no-eval', default=True, action='store_false', dest='eval')\n",
    "parser.add_argument('-o', '--outfile', default='output.log', type=argparse.FileType('a', encoding='ascii'))\n",
    "\n",
    "parser.add_argument('--clip_grad', default=False, action='store_true', dest='clip_grad')\n",
    "\n",
    "parser.add_argument('--model', type=str, choices=('VGG11_BN', 'VGG_SNIP', 'CNNNet', 'CIFAR10Net'),\n",
    "                    default='VGG11_BN', help='Dataset to use')\n",
    "\n",
    "parser.add_argument('--prune_strategy', type=str, choices=('None', 'SNIP', 'SNAP', 'random_masks', 'Iter-SNIP', 'Grasp'),\n",
    "                    default='None', help='Dataset to use')\n",
    "parser.add_argument('--prune_at_first_round', default=False, action='store_true', dest='prune_at_first_round')\n",
    "parser.add_argument('--keep_ratio', type=float, default=0.0,\n",
    "                    help='local client batch size')         \n",
    "parser.add_argument('--prune_vote', type=int, default=1,\n",
    "                    help='local client batch size')\n",
    "\n",
    "parser.add_argument('--single_shot_pruning', default=False, action='store_true', dest='single_shot_pruning')\n",
    "\n",
    "parser.add_argument('--partition_method', type=str, default='homo', metavar='N',\n",
    "                        help='how to partition the dataset on local workers')\n",
    "\n",
    "parser.add_argument('--partition_alpha', type=float, default=0.5, metavar='PA',\n",
    "                    help='partition alpha (default: 0.5)')\n",
    "\n",
    "parser.add_argument('--target_keep_ratio', default=0.1, type=float, help='server target keep ratio')\n",
    "\n",
    "parser.add_argument('--num_pruning_steps', type=int, help='total number of pruning steps')\n",
    "parser.add_argument('--pruning_steps_decay_mode', type=str, default='linear', choices=('linear', 'exp'), help='pruning steps decay mode')\n",
    "parser.add_argument('--saliency_mode', type=str, choices=('saliency', 'mask'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70641184",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=['--dataset', 'cifar10', \n",
    "                               '--eta', '0.01', \n",
    "                               '--device', '2', \n",
    "                               '--distribution', 'classic_iid', \n",
    "                               '--total-clients', '1', \n",
    "                               '--clients', '1', \n",
    "                               '--batch-size', '64', \n",
    "                               '--rounds', '100', \n",
    "                               '--model', 'VGG11_BN', \n",
    "                               '--prune_strategy', 'Grasp',\n",
    "                               '--epochs', '2',\n",
    "                               '--keep_ratio', '0.02',\n",
    "                               '--prune_vote', '1',\n",
    "                               '--prune_at_first_round',\n",
    "                               '--single_shot_pruning',\n",
    "                               '--partition_method', 'homo',\n",
    "                               '--partition_alpha', '0.5',\n",
    "                               '--target_keep_ratio', '0.02',\n",
    "                               '--num_pruning_steps', '1',\n",
    "                               '--pruning_steps_decay_mode', 'linear',\n",
    "                               '--saliency_mode', 'mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62671b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddbb79b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = fedsnip_obj.main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fff754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# print('...')\n",
    "# name = input()\n",
    "# print('++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88461634",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching dataset...\n",
      "INFO:root:*********partition data***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:traindata_cls_counts = {0: {0: 5000, 1: 5000, 2: 5000, 3: 5000, 4: 5000, 5: 5000, 6: 5000, 7: 5000, 8: 5000, 9: 5000}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000******************\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:train_dl_global number = 782\n",
      "INFO:root:test_dl_global number = 157\n",
      "INFO:root:client_idx = 0, local_sample_number = 50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 0, batch_num_train_local = 782, batch_num_test_local = 157\n",
      "Initializing clients...\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mslimfun\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/slimfun/fedsnip/runs/20oyebs2\" target=\"_blank\">FedAVG(d)Grasp-target_kr0.02-0.02-classic_iid-prune_at_1rTrue-single_shot_pruningTrue-lr0.01-clip_gradFalse</a></strong> to <a href=\"https://wandb.ai/slimfun/fedsnip\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params.data.shape: torch.Size([64, 3, 3, 3]); mask.shape: torch.Size([64, 3, 3, 3])\n",
      "params.data.shape: torch.Size([128, 64, 3, 3]); mask.shape: torch.Size([128, 64, 3, 3])\n",
      "params.data.shape: torch.Size([256, 128, 3, 3]); mask.shape: torch.Size([256, 128, 3, 3])\n",
      "params.data.shape: torch.Size([256, 256, 3, 3]); mask.shape: torch.Size([256, 256, 3, 3])\n",
      "params.data.shape: torch.Size([512, 256, 3, 3]); mask.shape: torch.Size([512, 256, 3, 3])\n",
      "params.data.shape: torch.Size([512, 512, 3, 3]); mask.shape: torch.Size([512, 512, 3, 3])\n",
      "params.data.shape: torch.Size([512, 512, 3, 3]); mask.shape: torch.Size([512, 512, 3, 3])\n",
      "params.data.shape: torch.Size([512, 512, 3, 3]); mask.shape: torch.Size([512, 512, 3, 3])\n",
      "server model param size: 295395648\n",
      "keep_ratio_steps: [0.020000000000000018]\n",
      "No post init specified in Grasp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fw/FL/efficient_FL/feddst/grasp.py:163: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(layer.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1): Iterations 0/1.\n",
      "(2): Iterations 0/1.\n",
      "(2): Iterations 1/1.\n",
      "** norm factor: tensor(1.0003e-06, device='cuda:2')\n",
      "** accept:  tensor(0.7526, device='cuda:2')\n",
      "tensor(184455, device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "# while True:\n",
    "#     debug_info = next(run)\n",
    "#     print(debug_info.msg)\n",
    "\n",
    "    \n",
    "#     input()\n",
    "debug_info = next(run)\n",
    "# masks = debug_info.obj\n",
    "# global_model = debug_info.obj\n",
    "# aggregated_masks = debug_info.obj[0]\n",
    "# cl_mask_prarms = debug_info.obj[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f0ab0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def check_masks(masks):\n",
    "    for m in masks:\n",
    "        print(m.mean())\n",
    "def statistic_masks(masks):\n",
    "    M = torch.cat([m.flatten() for m in masks]).to(torch.int)\n",
    "    print(M.shape)\n",
    "    print(torch.bincount(M))\n",
    "for k, v in debug_info.items():\n",
    "    print('*' * 10)\n",
    "    print(k)\n",
    "    check_masks(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9403d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = server.masks\n",
    "torch.cat(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = masks\n",
    "def count_vote(masks, vote):\n",
    "    tc = 0.\n",
    "    keeped = 0.\n",
    "    for i in range(len(masks)):\n",
    "        tc += masks[i].numel()\n",
    "        m = masks[i].clone().detach()\n",
    "        keeped += torch.sum(torch.where(m >= vote, 1, 0))\n",
    "\n",
    "    print(\"keeped: {}, total: {}, pct: {}\".format(keeped, tc, keeped/tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('vote {}'.format(i))\n",
    "    count_vote(server.masks, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = server.masks\n",
    "for i in range(len(masks)):\n",
    "    print(masks[i].shape)\n",
    "flat_masks = torch.cat([m.flatten() for m in masks])\n",
    "keep_num = len(flat_masks) * 0.1\n",
    "threshold, indices = flat_masks.topk(int(keep_num))\n",
    "global_masks = torch.zeros_like(flat_masks)\n",
    "global_masks[indices] = 1\n",
    "print(indices.sort())\n",
    "# len(indices)\n",
    "print(global_masks[:25])\n",
    "print(global_masks.sum())\n",
    "\n",
    "idx = 0\n",
    "ms = []\n",
    "for i in range(len(masks)):\n",
    "    m = global_masks[idx:idx+masks[i].numel()].reshape(masks[i].size())\n",
    "    idx += masks[i].numel()\n",
    "    print(m.shape)\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c954a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5])\n",
    "c = torch.tensor([6,7,8,9])\n",
    "ts = torch.cat([e.flatten() for e in [a,b,c]])\n",
    "print(ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba2be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_c = 0.0\n",
    "total = 0.0\n",
    "\n",
    "for name, param in server.model.state_dict().items():\n",
    "    a = param.view(-1).to(device='cpu', copy=True).numpy()\n",
    "    pruned_c +=sum(np.where(a, 0, 1))\n",
    "    total += param.numel()\n",
    "print(f'global model zero params: {pruned_c / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8968b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in server.model.state_dict():\n",
    "#     print(name)\n",
    "server.model = server.model.to('cuda:3')\n",
    "params = server.model.cpu().state_dict()\n",
    "print(params['features.0.weight'])\n",
    "\n",
    "print('***********'*3)\n",
    "\n",
    "# params['features.0.weight'] = torch.zeros_like(params['features.0.weight'])\n",
    "params['features.0.weight'][0][0][0] = 2.\n",
    "\n",
    "print(server.model.state_dict()['features.0.weight'])\n",
    "\n",
    "\n",
    "print(params['features.0.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d24b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_c = 0.0\n",
    "total = 0.0\n",
    "for name, param in global_model.state_dict().items():\n",
    "    a = param.view(-1).to(device='cpu', copy=True).numpy()\n",
    "    pruned_c +=sum(np.where(a, 0, 1))\n",
    "    total += param.numel()\n",
    "print(f'global model zero params: {pruned_c / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27829812",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_c = 0.\n",
    "total = 0.\n",
    "for name,mask in global_model.mask.items():\n",
    "#     print(mask)\n",
    "    prune_c += sum(np.where(mask.to('cpu', copy=True).view(-1).numpy(), 0, 1))\n",
    "    total += mask.numel()\n",
    "    \n",
    "print(prune_c / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d7b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb36d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "pruned_c = 0.0\n",
    "total = 0.0\n",
    "for name, param in global_model.state_dict().items():\n",
    "    a = param.view(-1).to(device='cpu', copy=True).numpy()\n",
    "    pruned_c +=sum(np.where(a, 0, 1))\n",
    "    total += param.numel()\n",
    "print(f'global model zero params: {pruned_c / total}')\n",
    "\n",
    "gcp_model = copy.deepcopy(global_model)\n",
    "\n",
    "\n",
    "\n",
    "prune_c = 0.\n",
    "total = 0.\n",
    "for name, params in gcp_model.state_dict().items():\n",
    "#     print(name)\n",
    "    prune_c += sum(np.where(params.to('cpu', copy=True).view(-1).numpy(), 0, 1))\n",
    "    total += params.numel()\n",
    "    \n",
    "print(prune_c / total)\n",
    "    \n",
    "# print('*'*10)\n",
    "# for name, mask in global_model.mask.items():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bca0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in aggregated_masks:\n",
    "#     print(name)\n",
    "#     print(aggregated_masks[name].dtype)\n",
    "\n",
    "def count_mask(state_dict):\n",
    "    non_zero = 0.\n",
    "    total = 0.\n",
    "    for name in state_dict:\n",
    "        non_zero += torch.count_nonzero(state_dict[name])\n",
    "        total += state_dict[name].numel()\n",
    "    return 1 - non_zero / total\n",
    "# a = aggregated_masks['features.4.weight']\n",
    "print(a.shape)\n",
    "# print(torch.count_nonzero(a))\n",
    "print(count_mask(aggregated_masks))\n",
    "mask = torch.where(a>=1, 1, 0)\n",
    "# print(torch.count_nonzero(mask))\n",
    "print(count_mask(cl_mask_prarms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "keep_masks = {0: [torch.Tensor([0, 1, 0, 1, 1])], 1: [torch.Tensor([1, 1, 0, 0, 1])]}\n",
    "\n",
    "model_list = [[2, torch.Tensor([1,2,3,4,5])], [3, torch.Tensor([1,2,3,4,5])]]\n",
    "\n",
    "masks = copy.deepcopy(keep_masks)\n",
    "for c, m in masks.items():\n",
    "    for i in range(len(m)):\n",
    "        m[i] *= model_list[c][0]\n",
    "    \n",
    "print(masks)\n",
    "\n",
    "total_masks = copy.deepcopy(masks)\n",
    "for c,m in total_masks.items():\n",
    "    if c!= 0:\n",
    "        for i in range(len(m)):\n",
    "            total_masks[0][i] += m[i]\n",
    "        \n",
    "print(total_masks)\n",
    "\n",
    "for c, m in masks.items():\n",
    "    for i in range(len(m)):\n",
    "        m[i] /= total_masks[0][i]\n",
    "        masks[c][i] = torch.where(torch.isnan(m[i]), torch.full_like(m[i], 0), m[i])\n",
    "#     print(torch.isnan(m))\n",
    "print(masks)\n",
    "#     m /= sum(model_list[i][0] for i in range(len(model_list)))\n",
    "\n",
    "# print(masks[0] * model_list[0][1])\n",
    "model_list[0][1] *= masks[0]\n",
    "# print(model_list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb781364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "list_tensor = [torch.Tensor([1,2]), torch.Tensor([2,3])]\n",
    "t = torch.stack(list_tensor)\n",
    "list_tensor *= 2\n",
    "list_tensor\n",
    "# print(t)\n",
    "# a = torch.from_numpy(np.asarray(list_tensor))\n",
    "# print(a)\n",
    "# torch.cat(list_tensor, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fc21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([2,3]).type(torch.float64)\n",
    "b = torch.Tensor([1,2]).type(torch.float64)\n",
    "print(a.dtype == b.dtype)\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(6, 2, bias=False),\n",
    "          nn.Sigmoid(),\n",
    "        )\n",
    "input = torch.randn(6)\n",
    "target = torch.randn(2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.1)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "output = model(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "print(list(model.parameters())[0].grad)\n",
    "optimizer.zero_grad() \n",
    "print(list(model.parameters())[0].grad)\n",
    "\n",
    "\n",
    "print(\"before step: \", list(model.parameters()))\n",
    "optimizer.step()\n",
    "print(\"after step: \", list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95506969",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_keep_ratio = 0.2\n",
    "num_pruning_steps = 2\n",
    "\n",
    "keep_ratio_steps = [1 - ((x + 1) * (1 - target_keep_ratio) / num_pruning_steps) for x in range(num_pruning_steps)]\n",
    "keep_ratio_steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
