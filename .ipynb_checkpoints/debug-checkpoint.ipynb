{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc456178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from scipy import test\n",
    "import torch\n",
    "import torch.cuda\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import argparse\n",
    "import gc\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "import wandb\n",
    "\n",
    "from datasets import get_dataset\n",
    "from models.models import all_models\n",
    "\n",
    "from client import Client\n",
    "from utils import *\n",
    "\n",
    "import fedsnip as fedsnip_obj\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "def device_list(x):\n",
    "    if x == 'cpu':\n",
    "        return [x]\n",
    "    return [int(y) for y in x.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2442c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--structure'], dest='structure', nargs=None, const=None, default=False, type=<class 'bool'>, choices=None, help='learning rate', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--eta', type=float, help='learning rate', default=0.01)\n",
    "parser.add_argument('--clients', type=int, help='number of clients per round', default=20)\n",
    "parser.add_argument('--rounds', type=int, help='number of global rounds', default=400)\n",
    "parser.add_argument('--epochs', type=int, help='number of local epochs', default=10)\n",
    "parser.add_argument('--dataset', type=str, choices=('mnist', 'emnist', 'cifar10', 'cifar100'),\n",
    "                    default='mnist', help='Dataset to use')\n",
    "parser.add_argument('--distribution', type=str, choices=('dirichlet', 'lotteryfl', 'iid', 'classic_iid'), default='dirichlet',\n",
    "                    help='how should the dataset be distributed?')\n",
    "parser.add_argument('--beta', type=float, default=0.1, help='Beta parameter (unbalance rate) for Dirichlet distribution')\n",
    "parser.add_argument('--total-clients', type=int, help='split the dataset between this many clients. Ignored for EMNIST.', default=400)\n",
    "parser.add_argument('--min-samples', type=int, default=0, help='minimum number of samples required to allow a client to participate')\n",
    "parser.add_argument('--samples-per-client', type=int, default=20, help='samples to allocate to each client (per class, for lotteryfl, or per client, for iid)')\n",
    "parser.add_argument('--prox', type=float, default=0, help='coefficient to proximal term (i.e. in FedProx)')\n",
    "\n",
    "parser.add_argument('--batch-size', type=int, default=32,\n",
    "                    help='local client batch size')\n",
    "parser.add_argument('--l2', default=1e-5, type=float, help='L2 regularization strength')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='Local client SGD momentum parameter')\n",
    "parser.add_argument('--cache-test-set', default=False, action='store_true', help='Load test sets into memory')\n",
    "parser.add_argument('--cache-test-set-gpu', default=False, action='store_true', help='Load test sets into GPU memory')\n",
    "parser.add_argument('--test-batches', default=0, type=int, help='Number of minibatches to test on, or 0 for all of them')\n",
    "parser.add_argument('--eval-every', default=1, type=int, help='Evaluate on test set every N rounds')\n",
    "parser.add_argument('--device', default='0', type=device_list, help='Device to use for compute. Use \"cpu\" to force CPU. Otherwise, separate with commas to allow multi-GPU.')\n",
    "parser.add_argument('--no-eval', default=True, action='store_false', dest='eval')\n",
    "parser.add_argument('-o', '--outfile', default='output.log', type=argparse.FileType('a', encoding='ascii'))\n",
    "\n",
    "parser.add_argument('--clip_grad', default=False, action='store_true', dest='clip_grad')\n",
    "\n",
    "parser.add_argument('--model', type=str, choices=('VGG11_BN', 'VGG_SNIP', 'CNNNet', 'CIFAR10Net'),\n",
    "                    default='VGG11_BN', help='Dataset to use')\n",
    "\n",
    "parser.add_argument('--prune_strategy', type=str, choices=('None', 'SNIP', 'SNAP', 'random_masks', 'Iter-SNIP', 'Grasp', 'PreCrop'),\n",
    "                    default='None', help='Dataset to use')\n",
    "parser.add_argument('--prune_at_first_round', default=False, action='store_true', dest='prune_at_first_round')\n",
    "parser.add_argument('--keep_ratio', type=float, default=0.0,\n",
    "                    help='local client batch size')         \n",
    "parser.add_argument('--prune_vote', type=int, default=1,\n",
    "                    help='local client batch size')\n",
    "\n",
    "parser.add_argument('--single_shot_pruning', default=False, action='store_true', dest='single_shot_pruning')\n",
    "\n",
    "parser.add_argument('--partition_method', type=str, default='homo', metavar='N',\n",
    "                        help='how to partition the dataset on local workers')\n",
    "\n",
    "parser.add_argument('--partition_alpha', type=float, default=0.5, metavar='PA',\n",
    "                    help='partition alpha (default: 0.5)')\n",
    "\n",
    "parser.add_argument('--target_keep_ratio', default=0.1, type=float, help='server target keep ratio')\n",
    "\n",
    "parser.add_argument('--num_pruning_steps', type=int, help='total number of pruning steps')\n",
    "parser.add_argument('--pruning_steps_decay_mode', type=str, default='linear', choices=('linear', 'exp'), help='pruning steps decay mode')\n",
    "parser.add_argument('--saliency_mode', type=str, choices=('saliency', 'mask'))\n",
    "parser.add_argument('--structure', type=bool, help='learning rate', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70641184",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=['--dataset', 'cifar10', \n",
    "                               '--eta', '0.01', \n",
    "                               '--device', '2', \n",
    "                               '--distribution', 'classic_iid', \n",
    "                               '--total-clients', '3', \n",
    "                               '--clients', '3', \n",
    "                               '--batch-size', '64', \n",
    "                               '--rounds', '100', \n",
    "                               '--model', 'VGG11_BN', \n",
    "                               '--prune_strategy', 'PreCrop',\n",
    "                               '--epochs', '10',\n",
    "                               '--keep_ratio', '0.1',\n",
    "                               '--prune_vote', '1',\n",
    "                               '--prune_at_first_round',\n",
    "                               '--single_shot_pruning',\n",
    "                               '--partition_method', 'hetero',\n",
    "                               '--partition_alpha', '0.5',\n",
    "                               '--target_keep_ratio', '0.1',\n",
    "                               '--num_pruning_steps', '1',\n",
    "                               '--pruning_steps_decay_mode', 'linear',\n",
    "                               '--structure', 'True',\n",
    "                               '--saliency_mode', 'mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62671b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddbb79b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = fedsnip_obj.main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0fff754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# print('...')\n",
    "# name = input()\n",
    "# print('++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88461634",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching dataset...\n",
      "INFO:root:*********partition data***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:N = 50000\n",
      "/home/fw/FL/efficient_FL/feddst/data_loader.py:234: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_idx = np.array(np.array_split(test_idx, n_nets))\n",
      "INFO:root:traindata_cls_counts = {0: {0: 4873, 1: 261, 2: 1161, 3: 4184, 4: 1683, 5: 230, 6: 106, 7: 1919, 8: 3849}, 1: {0: 82, 1: 906, 2: 423, 3: 672, 4: 2128, 5: 313, 6: 4072, 7: 370, 8: 213, 9: 5000}, 2: {0: 45, 1: 3833, 2: 3416, 3: 144, 4: 1189, 5: 4457, 6: 822, 7: 2711, 8: 938}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000******************\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:train_dl_global number = 782\n",
      "INFO:root:test_dl_global number = 157\n",
      "INFO:root:client_idx = 0, local_sample_number = 18266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 0, batch_num_train_local = 286, batch_num_test_local = 53\n",
      "INFO:root:client_idx = 1, local_sample_number = 14179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 1, batch_num_train_local = 222, batch_num_test_local = 53\n",
      "INFO:root:client_idx = 2, local_sample_number = 17555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 2, batch_num_train_local = 275, batch_num_test_local = 53\n",
      "Initializing clients...\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mslimfun\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/slimfun/fedsnip/runs/1b38yv5o\" target=\"_blank\">FedAVG(d)PreCrop-target_kr0.1-0.1-classic_iid-prune_at_1rTrue-single_shot_pruningTrue-lr0.01-clip_gradFalse</a></strong> to <a href=\"https://wandb.ai/slimfun/fedsnip\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "params.data.shape: torch.Size([64, 3, 3, 3]); mask.shape: torch.Size([64, 3, 3, 3])\n",
      "params.data.shape: torch.Size([128, 64, 3, 3]); mask.shape: torch.Size([128, 64, 3, 3])\n",
      "params.data.shape: torch.Size([256, 128, 3, 3]); mask.shape: torch.Size([256, 128, 3, 3])\n",
      "params.data.shape: torch.Size([256, 256, 3, 3]); mask.shape: torch.Size([256, 256, 3, 3])\n",
      "params.data.shape: torch.Size([512, 256, 3, 3]); mask.shape: torch.Size([512, 256, 3, 3])\n",
      "params.data.shape: torch.Size([512, 512, 3, 3]); mask.shape: torch.Size([512, 512, 3, 3])\n",
      "params.data.shape: torch.Size([512, 512, 3, 3]); mask.shape: torch.Size([512, 512, 3, 3])\n",
      "params.data.shape: torch.Size([512, 512, 3, 3]); mask.shape: torch.Size([512, 512, 3, 3])\n",
      "params.data.shape: torch.Size([10, 512]); mask.shape: torch.Size([10, 512])\n",
      "server model param size: 295395648\n",
      "No post init specified in PreCrop\n",
      "client 0 output_channels: [64, 127, 94, 203, 59, 322, 37, 512]\n",
      "client 0 layer_ratio [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0 : 1.0\n",
      "1 : 1.0\n",
      "2 : 1.0\n",
      "3 : 1.0\n",
      "4 : 1.0\n",
      "5 : 1.0\n",
      "6 : 1.0\n",
      "7 : 1.0\n",
      "client 1 output_channels: [64, 128, 205, 203, 304, 322, 292, 512]\n",
      "client 1 layer_ratio [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0 : 1.0\n",
      "1 : 1.0\n",
      "2 : 1.0\n",
      "3 : 1.0\n",
      "4 : 1.0\n",
      "5 : 1.0\n",
      "6 : 1.0\n",
      "7 : 1.0\n",
      "client 2 output_channels: [64, 127, 135, 203, 219, 322, 228, 512]\n",
      "client 2 layer_ratio [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0 : 1.0\n",
      "1 : 1.0\n",
      "2 : 1.0\n",
      "3 : 1.0\n",
      "4 : 1.0\n",
      "5 : 1.0\n",
      "6 : 1.0\n",
      "7 : 1.0\n",
      "None\n",
      "server_channels: [64, 128, 205, 203, 304, 322, 292, 512]\n",
      "reset_weights: [64, 127, 135, 203, 219, 322, 228, 512]\n",
      "===================\n",
      "features.0.weight\n",
      "features.4.weight\n",
      "features.8.weight\n",
      "features.11.weight\n",
      "features.15.weight\n",
      "features.18.weight\n",
      "features.22.weight\n",
      "features.25.weight\n",
      "**********test before train*************\n",
      "global model zero params: 0.001121081429508606\n",
      "Test client 2: Accuracy: 0.046824268996715546; Loss: 2.296825647354126; Total: 17555.0;\n",
      "accuracy: 0.046824268996715546; loss: 2.296825647354126\n",
      "**********test before train*************\n",
      "running loss: 1.3302092517505992\n",
      "running loss: 0.9686488385633989\n",
      "running loss: 0.8163601581616835\n",
      "running loss: 0.7101809371601452\n",
      "running loss: 0.6402625624700026\n",
      "running loss: 0.5916373570398851\n",
      "running loss: 0.5420658890767531\n",
      "running loss: 0.5042871883240613\n",
      "running loss: 0.4721386818452315\n",
      "running loss: 0.4428214212439277\n",
      "total: 175550.0; time: 4.370767974853516\n",
      "training time: 33.67391061782837\n",
      "**********test after train*************\n",
      "global model zero params: 0.0\n",
      "Test client 2: Accuracy: 0.8367986679077148; Loss: 0.48123204708099365; Total: 17555.0;\n",
      "accuracy: 0.8367986679077148; loss: 0.48123204708099365\n",
      "**********test after train*************\n",
      "client 2 finish params zeros: 0.0\n",
      "dl_cost: 0; ul_cost: 0\n",
      "17555\n",
      "reset_weights: [64, 127, 94, 203, 59, 322, 37, 512]\n",
      "===================\n",
      "features.0.weight\n",
      "features.4.weight\n",
      "features.8.weight\n",
      "features.11.weight\n",
      "features.15.weight\n",
      "features.18.weight\n",
      "features.22.weight\n",
      "features.25.weight\n",
      "**********test before train*************\n",
      "global model zero params: 0.0030819746744908077\n",
      "Test client 0: Accuracy: 0.005803131498396397; Loss: 2.321258783340454; Total: 18266.0;\n",
      "accuracy: 0.005803131498396397; loss: 2.321258783340454\n",
      "**********test before train*************\n",
      "running loss: 1.4372089242601729\n",
      "running loss: 1.0791726097777172\n",
      "running loss: 0.9418474546262434\n",
      "running loss: 0.8464112503753676\n",
      "running loss: 0.7813899539984189\n",
      "running loss: 0.7304493706751537\n",
      "running loss: 0.675249256037332\n",
      "running loss: 0.633336177551663\n",
      "running loss: 0.6019391221599979\n",
      "running loss: 0.5627523331375389\n",
      "total: 182660.0; time: 4.4885173559188845\n",
      "training time: 34.240403175354004\n",
      "**********test after train*************\n",
      "global model zero params: 0.0\n",
      "Test client 0: Accuracy: 0.8086061477661133; Loss: 0.5671985745429993; Total: 18266.0;\n",
      "accuracy: 0.8086061477661133; loss: 0.5671985745429993\n",
      "**********test after train*************\n",
      "client 0 finish params zeros: 0.0\n",
      "dl_cost: 0; ul_cost: 0\n",
      "18266\n",
      "reset_weights: [64, 128, 205, 203, 304, 322, 292, 512]\n",
      "===================\n",
      "features.0.weight\n",
      "features.4.weight\n",
      "features.8.weight\n",
      "features.11.weight\n",
      "features.15.weight\n",
      "features.18.weight\n",
      "features.22.weight\n",
      "features.25.weight\n",
      "**********test before train*************\n",
      "global model zero params: 0.0009395806670884155\n",
      "Test client 1: Accuracy: 0.2871852517127991; Loss: 2.2866787910461426; Total: 14179.0;\n",
      "accuracy: 0.2871852517127991; loss: 2.2866787910461426\n",
      "**********test before train*************\n",
      "running loss: 1.301235159506669\n",
      "running loss: 0.94684719676907\n",
      "running loss: 0.7988305895983636\n",
      "running loss: 0.7149818715479996\n",
      "running loss: 0.6476862762156907\n",
      "running loss: 0.5892327639299471\n",
      "running loss: 0.5559690919545319\n",
      "running loss: 0.5169005706772074\n",
      "running loss: 0.47619587928056717\n",
      "running loss: 0.44682838405306274\n",
      "total: 141790.0; time: 3.7445887088775636\n",
      "training time: 28.299417972564697\n",
      "**********test after train*************\n",
      "global model zero params: 0.0\n",
      "Test client 1: Accuracy: 0.8377177119255066; Loss: 0.4860227704048157; Total: 14179.0;\n",
      "accuracy: 0.8377177119255066; loss: 0.4860227704048157\n",
      "**********test after train*************\n",
      "client 1 finish params zeros: 4.6193739778191517e-07\n",
      "dl_cost: 0; ul_cost: 0\n",
      "14179\n",
      "output_channel[0] -- 64, prio_channel: 3\n",
      "output_channel[0] -- 64, prio_channel: 3\n",
      "output_channel[0] -- 64, prio_channel: 3\n",
      "output_channel[1] -- 127, prio_channel: 64\n",
      "output_channel[1] -- 127, prio_channel: 64\n",
      "output_channel[1] -- 128, prio_channel: 64\n",
      "output_channel[2] -- 135, prio_channel: 127\n",
      "output_channel[2] -- 94, prio_channel: 127\n",
      "output_channel[2] -- 205, prio_channel: 128\n",
      "output_channel[3] -- 203, prio_channel: 135\n",
      "output_channel[3] -- 203, prio_channel: 94\n",
      "output_channel[3] -- 203, prio_channel: 205\n",
      "output_channel[4] -- 219, prio_channel: 203\n",
      "output_channel[4] -- 59, prio_channel: 203\n",
      "output_channel[4] -- 304, prio_channel: 203\n",
      "output_channel[5] -- 322, prio_channel: 219\n",
      "output_channel[5] -- 322, prio_channel: 59\n",
      "output_channel[5] -- 322, prio_channel: 304\n",
      "output_channel[6] -- 228, prio_channel: 322\n",
      "output_channel[6] -- 37, prio_channel: 322\n",
      "output_channel[6] -- 292, prio_channel: 322\n",
      "output_channel[7] -- 512, prio_channel: 228\n",
      "output_channel[7] -- 512, prio_channel: 37\n",
      "output_channel[7] -- 512, prio_channel: 292\n"
     ]
    }
   ],
   "source": [
    "# while True:\n",
    "#     debug_info = next(run)\n",
    "#     print(debug_info.msg)\n",
    "\n",
    "    \n",
    "#     input()\n",
    "debug_info = next(run)\n",
    "# masks = debug_info.obj\n",
    "# global_model = debug_info.obj\n",
    "# aggregated_masks = debug_info.obj[0]\n",
    "# cl_mask_prarms = debug_info.obj[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f51e167",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ['features.0.weight', 'features.0.bias', 'features.1.weight', 'features.1.bias', 'features.1.running_mean', 'features.1.running_var', 'features.4.weight', 'features.4.bias', 'features.5.weight', 'features.5.bias', 'features.5.running_mean', 'features.5.running_var', 'features.8.weight', 'features.8.bias', 'features.9.weight', 'features.9.bias', 'features.9.running_mean', 'features.9.running_var', 'features.11.weight', 'features.11.bias', 'features.12.weight', 'features.12.bias', 'features.12.running_mean', 'features.12.running_var', 'features.15.weight', 'features.15.bias', 'features.16.weight', 'features.16.bias', 'features.16.running_mean', 'features.16.running_var', 'features.18.weight', 'features.18.bias', 'features.19.weight', 'features.19.bias', 'features.19.running_mean', 'features.19.running_var', 'features.22.weight', 'features.22.bias', 'features.23.weight', 'features.23.bias', 'features.23.running_mean', 'features.23.running_var', 'features.25.weight', 'features.25.bias', 'features.26.weight', 'features.26.bias', 'features.26.running_mean', 'features.26.running_var', 'classifier.fc1.weight', 'classifier.fc1.bias']\n",
    "# 28211 50000\n",
    "# debug_info['features.11.weight'][:204,:95,:,:]\n",
    "# model_list, client_idx, server_params = debug_info\n",
    "training_nums = debug_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c4fc491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight: torch.Size([64, 3, 3, 3])\n",
      "features.0.bias: torch.Size([64])\n",
      "features.1.weight: torch.Size([64])\n",
      "features.1.bias: torch.Size([64])\n",
      "features.1.running_mean: torch.Size([64])\n",
      "features.1.running_var: torch.Size([64])\n",
      "features.4.weight: torch.Size([128, 64, 3, 3])\n",
      "features.4.bias: torch.Size([128])\n",
      "features.5.weight: torch.Size([128])\n",
      "features.5.bias: torch.Size([128])\n",
      "features.5.running_mean: torch.Size([128])\n",
      "features.5.running_var: torch.Size([128])\n",
      "features.8.weight: torch.Size([205, 128, 3, 3])\n",
      "features.8.bias: torch.Size([205])\n",
      "features.9.weight: torch.Size([205])\n",
      "features.9.bias: torch.Size([205])\n",
      "features.9.running_mean: torch.Size([205])\n",
      "features.9.running_var: torch.Size([205])\n",
      "features.11.weight: torch.Size([203, 205, 3, 3])\n",
      "features.11.bias: torch.Size([203])\n",
      "features.12.weight: torch.Size([203])\n",
      "features.12.bias: torch.Size([203])\n",
      "features.12.running_mean: torch.Size([203])\n",
      "features.12.running_var: torch.Size([203])\n",
      "features.15.weight: torch.Size([304, 203, 3, 3])\n",
      "features.15.bias: torch.Size([304])\n",
      "features.16.weight: torch.Size([304])\n",
      "features.16.bias: torch.Size([304])\n",
      "features.16.running_mean: torch.Size([304])\n",
      "features.16.running_var: torch.Size([304])\n",
      "features.18.weight: torch.Size([322, 304, 3, 3])\n",
      "features.18.bias: torch.Size([322])\n",
      "features.19.weight: torch.Size([322])\n",
      "features.19.bias: torch.Size([322])\n",
      "features.19.running_mean: torch.Size([322])\n",
      "features.19.running_var: torch.Size([322])\n",
      "features.22.weight: torch.Size([292, 322, 3, 3])\n",
      "features.22.bias: torch.Size([292])\n",
      "features.23.weight: torch.Size([292])\n",
      "features.23.bias: torch.Size([292])\n",
      "features.23.running_mean: torch.Size([292])\n",
      "features.23.running_var: torch.Size([292])\n",
      "features.25.weight: torch.Size([512, 292, 3, 3])\n",
      "features.25.bias: torch.Size([512])\n",
      "features.26.weight: torch.Size([512])\n",
      "features.26.bias: torch.Size([512])\n",
      "features.26.running_mean: torch.Size([512])\n",
      "features.26.running_var: torch.Size([512])\n",
      "classifier.fc1.weight: torch.Size([10, 512])\n",
      "classifier.fc1.bias: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for k, v in training_nums.items():\n",
    "    print(f'{k}: {v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dbfadc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _WandbInit._resume_backend at 0x7f57b3883050> (for pre_run_cell):\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The wandb backend process has shutdown",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/fedml/lib/python3.7/site-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml/lib/python3.7/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36m_resume_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resuming backend\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jupyter_teardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml/lib/python3.7/site-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mpublish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpublish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mresume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResumeRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResumeRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRunRecord\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml/lib/python3.7/site-packages/wandb/sdk/interface/interface_queue.py\u001b[0m in \u001b[0;36m_publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pb.Record\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_check\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The wandb backend process has shutdown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for dimension 2 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2234/4025547170.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_nums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features.22.weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m37\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for dimension 2 with size 3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _WandbInit._pause_backend at 0x7f57c6ab3c20> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The wandb backend process has shutdown",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/fedml/lib/python3.7/site-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml/lib/python3.7/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36m_pause_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved code: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_resume_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml/lib/python3.7/site-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mpublish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpublish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mpause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPauseRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpause\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPauseRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResumeRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml/lib/python3.7/site-packages/wandb/sdk/interface/interface_queue.py\u001b[0m in \u001b[0;36m_publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pb.Record\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_check\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The wandb backend process has shutdown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown"
     ]
    }
   ],
   "source": [
    "training_nums['features.22.weight'][37,0,3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e69fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_list[3][2]\n",
    "client_idx\n",
    "\n",
    "# 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9a9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scale(model_params):\n",
    "    for k, p in model_params.items():\n",
    "        if len(p.shape) == 0:\n",
    "            continue\n",
    "        print(f'{k}: {p.shape}, {p.mean()}')\n",
    "        \n",
    "\n",
    "model = torch.load('client_model_9.pt')\n",
    "\n",
    "weight_scale(model.state_dict())\n",
    "# weight_scale(model_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb7fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_model(model_params):\n",
    "    return torch.cat([p.flatten() for p in model_params.values()])\n",
    "i = 1\n",
    "for i in range(len(model_list)):\n",
    "    print(torch.cosine_similarity(flatten_model(model_list[i-1]), flatten_model(model_list[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12da2766",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def exchange_dim(model_params):\n",
    "    prio_exchange_idx = [0,1,2]\n",
    "    for k, p in model_params.items():\n",
    "        if len(p.shape) == 4:\n",
    "#             exchange_idx = []\n",
    "#             for i in range(p.shape[0])\n",
    "            exchange_idx = np.argsort([p[i,:,:,:].mean().cpu() for i in range(p.shape[0])])\n",
    "#             print(exchange_idx)\n",
    "            model_params[k] = model_params[k][:,prio_exchange_idx,:,:]\n",
    "            model_params[k] = model_params[k][exchange_idx,:,:,:]\n",
    "            prio_exchange_idx = exchange_idx\n",
    "        elif len(p.shape) == 1:\n",
    "#             print(p.shape)\n",
    "#             print(prio_exchange_idx)\n",
    "            model_params[k] = model_params[k][prio_exchange_idx]\n",
    "        elif len(p.shape) == 2:\n",
    "            model_params[k] = model_params[k][:,prio_exchange_idx]\n",
    "            prio_exchange_idx = [i for i in range(10)]\n",
    "            \n",
    "exchange_dim(vgg11_pruned.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b5499",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11_pruned = all_models['VGG11_BN']('cuda:3', output_channels=[64, 128, 95, 204, 60, 322, 38, 512]).to('cuda:3')\n",
    "# vgg11_pruned.load_state_dict(model_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f1ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_scale(model_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b684763",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_scale(model_list[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b161b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11_full = all_models['VGG11_BN']('cuda:3', output_channels=[64, 128, 256, 256, 512, 512, 512, 512]).to('cuda:3')\n",
    "vgg11_full.load_state_dict(model_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fbc67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_model = all_models['VGG11_BN']('cuda:3', output_channels=[64, 128, 256, 256, 512, 512, 512, 512]).to('cuda:3')\n",
    "server_model.load_state_dict(server_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c94049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import load_partition_data_cifar10, get_dataloader_test_CIFAR10, get_dataloader\n",
    "import os\n",
    "\n",
    "path = os.path.join('..', 'data', args.dataset)\n",
    "_, test_data_global = get_dataloader(args.dataset, path, 64, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375f8dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = os.path.join('..', 'data', args.dataset)\n",
    "train_data_num, test_data_num, train_data_global, test_data_global, \\\n",
    "    train_data_local_num_dict, train_data_local_dict, test_data_local_dict, \\\n",
    "    class_num = load_partition_data_cifar10(args.dataset, path, args.partition_method, args.partition_alpha, args.total_clients, args.batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756103f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([5971,5081,5248,5944,3876])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69041fbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def test(model=None, n_batches=0, test_loader=None):\n",
    "        '''Evaluate the local model on the local test set.\n",
    "\n",
    "        model - model to evaluate, or this client's model if None\n",
    "        n_batches - number of minibatches to test on, or 0 for all of them\n",
    "        '''\n",
    "\n",
    "        correct = 0.\n",
    "        total = 0.\n",
    "        loss = 0.\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss().to(model.device)\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(test_loader):\n",
    "                if i > n_batches and n_batches > 0:\n",
    "                    break\n",
    "                inputs = inputs.to(model.device)\n",
    "                labels = labels.to(model.device)\n",
    "                outputs = model(inputs)\n",
    "#                 print(outputs)\n",
    "                loss += criterion(outputs, labels) * len(labels)\n",
    "                outputs = torch.argmax(outputs, dim=-1)\n",
    "                correct += sum(labels == outputs)\n",
    "                total += len(labels)\n",
    "\n",
    "        # remove copies if needed\n",
    "        # if model is not _model:\n",
    "        #     del _model\n",
    "\n",
    "        print(f'Test : Accuracy: {correct / total}; Loss: {loss / total}; Total: {total};')\n",
    "\n",
    "        return correct / total, loss / total\n",
    "\n",
    "# Test : Accuracy: 0.42100003361701965; Loss: 2.1450095176696777; Total: 1000.0;\n",
    "# Test : Accuracy: 0.44600000977516174; Loss: 2.139613389968872; Total: 1000.0;\n",
    "# Test : Accuracy: 0.39000001549720764; Loss: 2.363382577896118; Total: 1000.0;\n",
    "# Test : Accuracy: 0.40800002217292786; Loss: 2.3720834255218506; Total: 1000.0;\n",
    "# Test : Accuracy: 0.3930000066757202; Loss: 2.366488456726074; Total: 1000.0;\n",
    "# Test : Accuracy: 0.4000000059604645; Loss: 2.2676665782928467; Total: 1000.0;\n",
    "# Test : Accuracy: 0.4180000126361847; Loss: 2.3040504455566406; Total: 1000.0;\n",
    "# Test : Accuracy: 0.44700002670288086; Loss: 2.065028667449951; Total: 1000.0;\n",
    "# Test : Accuracy: 0.3760000169277191; Loss: 2.3444175720214844; Total: 1000.0;\n",
    "# Test : Accuracy: 0.3890000283718109; Loss: 2.352550506591797; Total: 1000.0;\n",
    "for i in range(10):\n",
    "    test(vgg11_pruned, test_loader=test_data_local_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce80cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "recons_model = all_models['VGG11_BN']('cuda:3', output_channels=[64, 128, 95, 204, 60, 322, 38, 512]).to('cuda:3')\n",
    "def reset_weights(net, global_state_dict, output_channels):\n",
    "    print('===================')\n",
    "    prio_channel = 3\n",
    "    local_params = net.state_dict()\n",
    "    idx = -1\n",
    "    for k in local_params.keys():\n",
    "        shape_dim = len(local_params[k].shape)\n",
    "        # print(local_params[k].shape)\n",
    "        if shape_dim == 4:\n",
    "            idx += 1\n",
    "            # print(f'{local_params[k].shape} == {self.output_channels[idx]}')\n",
    "            local_params[k].copy_(global_state_dict[k][:output_channels[idx],:prio_channel,:,:])\n",
    "            prio_channel = output_channels[idx]\n",
    "        elif shape_dim == 1:\n",
    "            # print(local_params[k].shape)\n",
    "            local_params[k].copy_(global_state_dict[k][:output_channels[idx]])\n",
    "        elif shape_dim == 2:\n",
    "            # print(f'{local_params[k].shape} == {self.output_channels[idx]}')\n",
    "            local_params[k].copy_(global_state_dict[k][:,:output_channels[idx]])\n",
    "\n",
    "reset_weights(recons_model, server_params, output_channels=[64, 128, 95, 204, 60, 322, 38, 512])\n",
    "\n",
    "for i in range(10):\n",
    "    test(recons_model, test_loader=test_data_local_dict[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f0ab0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "1/0\n",
    "def check_masks(masks):\n",
    "    for m in masks:\n",
    "        print(m.mean())\n",
    "def statistic_masks(masks):\n",
    "    M = torch.cat([m.flatten() for m in masks]).to(torch.int)\n",
    "    print(M.shape)\n",
    "    print(torch.bincount(M))\n",
    "for k, v in debug_info.items():\n",
    "    print('*' * 10)\n",
    "    print(k)\n",
    "    check_masks(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9403d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = server.masks\n",
    "torch.cat(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = masks\n",
    "def count_vote(masks, vote):\n",
    "    tc = 0.\n",
    "    keeped = 0.\n",
    "    for i in range(len(masks)):\n",
    "        tc += masks[i].numel()\n",
    "        m = masks[i].clone().detach()\n",
    "        keeped += torch.sum(torch.where(m >= vote, 1, 0))\n",
    "\n",
    "    print(\"keeped: {}, total: {}, pct: {}\".format(keeped, tc, keeped/tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('vote {}'.format(i))\n",
    "    count_vote(server.masks, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = server.masks\n",
    "for i in range(len(masks)):\n",
    "    print(masks[i].shape)\n",
    "flat_masks = torch.cat([m.flatten() for m in masks])\n",
    "keep_num = len(flat_masks) * 0.1\n",
    "threshold, indices = flat_masks.topk(int(keep_num))\n",
    "global_masks = torch.zeros_like(flat_masks)\n",
    "global_masks[indices] = 1\n",
    "print(indices.sort())\n",
    "# len(indices)\n",
    "print(global_masks[:25])\n",
    "print(global_masks.sum())\n",
    "\n",
    "idx = 0\n",
    "ms = []\n",
    "for i in range(len(masks)):\n",
    "    m = global_masks[idx:idx+masks[i].numel()].reshape(masks[i].size())\n",
    "    idx += masks[i].numel()\n",
    "    print(m.shape)\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c954a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5])\n",
    "c = torch.tensor([6,7,8,9])\n",
    "ts = torch.cat([e.flatten() for e in [a,b,c]])\n",
    "print(ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba2be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_c = 0.0\n",
    "total = 0.0\n",
    "\n",
    "for name, param in server.model.state_dict().items():\n",
    "    a = param.view(-1).to(device='cpu', copy=True).numpy()\n",
    "    pruned_c +=sum(np.where(a, 0, 1))\n",
    "    total += param.numel()\n",
    "print(f'global model zero params: {pruned_c / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8968b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in server.model.state_dict():\n",
    "#     print(name)\n",
    "server.model = server.model.to('cuda:3')\n",
    "params = server.model.cpu().state_dict()\n",
    "print(params['features.0.weight'])\n",
    "\n",
    "print('***********'*3)\n",
    "\n",
    "# params['features.0.weight'] = torch.zeros_like(params['features.0.weight'])\n",
    "params['features.0.weight'][0][0][0] = 2.\n",
    "\n",
    "print(server.model.state_dict()['features.0.weight'])\n",
    "\n",
    "\n",
    "print(params['features.0.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d24b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_c = 0.0\n",
    "total = 0.0\n",
    "for name, param in global_model.state_dict().items():\n",
    "    a = param.view(-1).to(device='cpu', copy=True).numpy()\n",
    "    pruned_c +=sum(np.where(a, 0, 1))\n",
    "    total += param.numel()\n",
    "print(f'global model zero params: {pruned_c / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27829812",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_c = 0.\n",
    "total = 0.\n",
    "for name,mask in global_model.mask.items():\n",
    "#     print(mask)\n",
    "    prune_c += sum(np.where(mask.to('cpu', copy=True).view(-1).numpy(), 0, 1))\n",
    "    total += mask.numel()\n",
    "    \n",
    "print(prune_c / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d7b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb36d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "pruned_c = 0.0\n",
    "total = 0.0\n",
    "for name, param in global_model.state_dict().items():\n",
    "    a = param.view(-1).to(device='cpu', copy=True).numpy()\n",
    "    pruned_c +=sum(np.where(a, 0, 1))\n",
    "    total += param.numel()\n",
    "print(f'global model zero params: {pruned_c / total}')\n",
    "\n",
    "gcp_model = copy.deepcopy(global_model)\n",
    "\n",
    "\n",
    "\n",
    "prune_c = 0.\n",
    "total = 0.\n",
    "for name, params in gcp_model.state_dict().items():\n",
    "#     print(name)\n",
    "    prune_c += sum(np.where(params.to('cpu', copy=True).view(-1).numpy(), 0, 1))\n",
    "    total += params.numel()\n",
    "    \n",
    "print(prune_c / total)\n",
    "    \n",
    "# print('*'*10)\n",
    "# for name, mask in global_model.mask.items():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bca0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in aggregated_masks:\n",
    "#     print(name)\n",
    "#     print(aggregated_masks[name].dtype)\n",
    "\n",
    "def count_mask(state_dict):\n",
    "    non_zero = 0.\n",
    "    total = 0.\n",
    "    for name in state_dict:\n",
    "        non_zero += torch.count_nonzero(state_dict[name])\n",
    "        total += state_dict[name].numel()\n",
    "    return 1 - non_zero / total\n",
    "# a = aggregated_masks['features.4.weight']\n",
    "print(a.shape)\n",
    "# print(torch.count_nonzero(a))\n",
    "print(count_mask(aggregated_masks))\n",
    "mask = torch.where(a>=1, 1, 0)\n",
    "# print(torch.count_nonzero(mask))\n",
    "print(count_mask(cl_mask_prarms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "keep_masks = {0: [torch.Tensor([0, 1, 0, 1, 1])], 1: [torch.Tensor([1, 1, 0, 0, 1])]}\n",
    "\n",
    "model_list = [[2, torch.Tensor([1,2,3,4,5])], [3, torch.Tensor([1,2,3,4,5])]]\n",
    "\n",
    "masks = copy.deepcopy(keep_masks)\n",
    "for c, m in masks.items():\n",
    "    for i in range(len(m)):\n",
    "        m[i] *= model_list[c][0]\n",
    "    \n",
    "print(masks)\n",
    "\n",
    "total_masks = copy.deepcopy(masks)\n",
    "for c,m in total_masks.items():\n",
    "    if c!= 0:\n",
    "        for i in range(len(m)):\n",
    "            total_masks[0][i] += m[i]\n",
    "        \n",
    "print(total_masks)\n",
    "\n",
    "for c, m in masks.items():\n",
    "    for i in range(len(m)):\n",
    "        m[i] /= total_masks[0][i]\n",
    "        masks[c][i] = torch.where(torch.isnan(m[i]), torch.full_like(m[i], 0), m[i])\n",
    "#     print(torch.isnan(m))\n",
    "print(masks)\n",
    "#     m /= sum(model_list[i][0] for i in range(len(model_list)))\n",
    "\n",
    "# print(masks[0] * model_list[0][1])\n",
    "model_list[0][1] *= masks[0]\n",
    "# print(model_list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb781364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "list_tensor = [torch.Tensor([1,2]), torch.Tensor([2,3])]\n",
    "t = torch.stack(list_tensor)\n",
    "list_tensor *= 2\n",
    "list_tensor\n",
    "# print(t)\n",
    "# a = torch.from_numpy(np.asarray(list_tensor))\n",
    "# print(a)\n",
    "# torch.cat(list_tensor, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fc21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([2,3]).type(torch.float64)\n",
    "b = torch.Tensor([1,2]).type(torch.float64)\n",
    "print(a.dtype == b.dtype)\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(6, 2, bias=False),\n",
    "          nn.Sigmoid(),\n",
    "        )\n",
    "input = torch.randn(6)\n",
    "target = torch.randn(2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.1)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "output = model(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "print(list(model.parameters())[0].grad)\n",
    "optimizer.zero_grad() \n",
    "print(list(model.parameters())[0].grad)\n",
    "\n",
    "\n",
    "print(\"before step: \", list(model.parameters()))\n",
    "optimizer.step()\n",
    "print(\"after step: \", list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95506969",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_keep_ratio = 0.2\n",
    "num_pruning_steps = 2\n",
    "\n",
    "keep_ratio_steps = [1 - ((x + 1) * (1 - target_keep_ratio) / num_pruning_steps) for x in range(num_pruning_steps)]\n",
    "keep_ratio_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed20b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for r in range(100):\n",
    "    lr = learning_rate * 1/(1 + 0.1 * r)\n",
    "    print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054de4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "bandwidths = []\n",
    "for i in range(10):\n",
    "    bandwidths.append(random.uniform(1.5, 10))\n",
    "print(bandwidths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
