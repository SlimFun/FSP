{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc456178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from scipy import test\n",
    "import torch\n",
    "import torch.cuda\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import argparse\n",
    "import gc\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "import wandb\n",
    "\n",
    "from datasets import get_dataset\n",
    "from models.models import all_models\n",
    "\n",
    "from client import Client\n",
    "from utils import *\n",
    "\n",
    "import fedsnip as fedsnip_obj\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "def device_list(x):\n",
    "    if x == 'cpu':\n",
    "        return [x]\n",
    "    return [int(y) for y in x.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b2442c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--pruning_steps_decay_mode'], dest='pruning_steps_decay_mode', nargs=None, const=None, default='linear', type=<class 'str'>, choices=('linear', 'exp'), help='pruning steps decay mode', metavar=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--eta', type=float, help='learning rate', default=0.01)\n",
    "parser.add_argument('--clients', type=int, help='number of clients per round', default=20)\n",
    "parser.add_argument('--rounds', type=int, help='number of global rounds', default=400)\n",
    "parser.add_argument('--epochs', type=int, help='number of local epochs', default=10)\n",
    "parser.add_argument('--dataset', type=str, choices=('mnist', 'emnist', 'cifar10', 'cifar100'),\n",
    "                    default='mnist', help='Dataset to use')\n",
    "parser.add_argument('--distribution', type=str, choices=('dirichlet', 'lotteryfl', 'iid', 'classic_iid'), default='dirichlet',\n",
    "                    help='how should the dataset be distributed?')\n",
    "parser.add_argument('--beta', type=float, default=0.1, help='Beta parameter (unbalance rate) for Dirichlet distribution')\n",
    "parser.add_argument('--total-clients', type=int, help='split the dataset between this many clients. Ignored for EMNIST.', default=400)\n",
    "parser.add_argument('--min-samples', type=int, default=0, help='minimum number of samples required to allow a client to participate')\n",
    "parser.add_argument('--samples-per-client', type=int, default=20, help='samples to allocate to each client (per class, for lotteryfl, or per client, for iid)')\n",
    "parser.add_argument('--prox', type=float, default=0, help='coefficient to proximal term (i.e. in FedProx)')\n",
    "\n",
    "parser.add_argument('--batch-size', type=int, default=32,\n",
    "                    help='local client batch size')\n",
    "parser.add_argument('--l2', default=1e-5, type=float, help='L2 regularization strength')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='Local client SGD momentum parameter')\n",
    "parser.add_argument('--cache-test-set', default=False, action='store_true', help='Load test sets into memory')\n",
    "parser.add_argument('--cache-test-set-gpu', default=False, action='store_true', help='Load test sets into GPU memory')\n",
    "parser.add_argument('--test-batches', default=0, type=int, help='Number of minibatches to test on, or 0 for all of them')\n",
    "parser.add_argument('--eval-every', default=1, type=int, help='Evaluate on test set every N rounds')\n",
    "parser.add_argument('--device', default='0', type=device_list, help='Device to use for compute. Use \"cpu\" to force CPU. Otherwise, separate with commas to allow multi-GPU.')\n",
    "parser.add_argument('--no-eval', default=True, action='store_false', dest='eval')\n",
    "parser.add_argument('-o', '--outfile', default='output.log', type=argparse.FileType('a', encoding='ascii'))\n",
    "\n",
    "parser.add_argument('--clip_grad', default=False, action='store_true', dest='clip_grad')\n",
    "\n",
    "parser.add_argument('--model', type=str, choices=('VGG11_BN', 'VGG_SNIP', 'CNNNet', 'CIFAR10Net'),\n",
    "                    default='VGG11_BN', help='Dataset to use')\n",
    "\n",
    "parser.add_argument('--prune_strategy', type=str, choices=('None', 'SNIP', 'SNAP', 'random_masks', 'Iter-SNIP'),\n",
    "                    default='None', help='Dataset to use')\n",
    "parser.add_argument('--prune_at_first_round', default=False, action='store_true', dest='prune_at_first_round')\n",
    "parser.add_argument('--keep_ratio', type=float, default=0.0,\n",
    "                    help='local client batch size')         \n",
    "parser.add_argument('--prune_vote', type=int, default=1,\n",
    "                    help='local client batch size')\n",
    "\n",
    "parser.add_argument('--single_shot_pruning', default=False, action='store_true', dest='single_shot_pruning')\n",
    "\n",
    "parser.add_argument('--partition_method', type=str, default='homo', metavar='N',\n",
    "                        help='how to partition the dataset on local workers')\n",
    "\n",
    "parser.add_argument('--partition_alpha', type=float, default=0.5, metavar='PA',\n",
    "                    help='partition alpha (default: 0.5)')\n",
    "\n",
    "parser.add_argument('--target_keep_ratio', default=0.1, type=float, help='server target keep ratio')\n",
    "\n",
    "parser.add_argument('--num_pruning_steps', type=int, help='total number of pruning steps')\n",
    "parser.add_argument('--pruning_steps_decay_mode', type=str, default='linear', choices=('linear', 'exp'), help='pruning steps decay mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70641184",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=['--dataset', 'cifar10', \n",
    "                               '--eta', '0.01', \n",
    "                               '--device', '3', \n",
    "                               '--distribution', 'classic_iid', \n",
    "                               '--total-clients', '10', \n",
    "                               '--clients', '10', \n",
    "                               '--batch-size', '64', \n",
    "                               '--rounds', '100', \n",
    "                               '--model', 'CNNNet', \n",
    "                               '--prune_strategy', 'Iter-SNIP',\n",
    "                               '--epochs', '2',\n",
    "                               '--keep_ratio', '0.1',\n",
    "                               '--prune_vote', '1',\n",
    "                               '--prune_at_first_round',\n",
    "                               '--single_shot_pruning',\n",
    "                               '--partition_method', 'hetero',\n",
    "                               '--partition_alpha', '0.5',\n",
    "                               '--target_keep_ratio', '0.2',\n",
    "                               '--num_pruning_steps', '2',\n",
    "                               '--pruning_steps_decay_mode', 'linear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62671b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddbb79b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching dataset...\n",
      "INFO:root:*********partition data***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:N = 50000\n",
      "INFO:root:traindata_cls_counts = {0: {1: 1088, 2: 6, 3: 250, 4: 298, 5: 2944, 6: 257, 7: 1, 8: 94, 9: 1033}, 1: {0: 314, 1: 128, 2: 594, 3: 44, 4: 977, 5: 650, 6: 16, 7: 1205, 8: 19, 9: 222}, 2: {0: 1059, 1: 1042, 2: 3, 3: 7, 4: 10, 5: 142, 6: 1428, 7: 958, 8: 432}, 3: {0: 2, 1: 683, 2: 423, 3: 111, 4: 61, 7: 43, 8: 849, 9: 2277}, 4: {0: 1062, 1: 34, 2: 1832, 3: 1, 4: 277, 5: 460, 6: 69, 7: 90, 8: 990, 9: 433}, 5: {0: 397, 1: 5, 2: 1416, 3: 1176, 4: 906, 5: 35, 6: 220, 7: 215, 8: 175, 9: 593}, 6: {0: 210, 1: 434, 2: 10, 3: 818, 4: 259, 6: 1312, 7: 718, 8: 2183}, 7: {0: 265, 1: 1329, 2: 11, 3: 18, 4: 521, 5: 426, 6: 22, 7: 1624, 8: 156, 9: 407}, 8: {0: 1380, 1: 64, 2: 206, 3: 213, 4: 1683, 5: 1, 6: 47, 7: 145, 8: 102, 9: 35}, 9: {0: 311, 1: 193, 2: 499, 3: 2362, 4: 8, 5: 342, 6: 1629, 7: 1}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000******************\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:train_dl_global number = 782\n",
      "INFO:root:test_dl_global number = 157\n",
      "INFO:root:client_idx = 0, local_sample_number = 5971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 0, batch_num_train_local = 94, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 1, local_sample_number = 4169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 1, batch_num_train_local = 66, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 2, local_sample_number = 5081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 2, batch_num_train_local = 80, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 3, local_sample_number = 4449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 3, batch_num_train_local = 70, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 4, local_sample_number = 5248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 4, batch_num_train_local = 82, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 5, local_sample_number = 5138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 5, batch_num_train_local = 81, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 6, local_sample_number = 5944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 6, batch_num_train_local = 93, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 7, local_sample_number = 4779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 7, batch_num_train_local = 75, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 8, local_sample_number = 3876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 8, batch_num_train_local = 61, batch_num_test_local = 16\n",
      "INFO:root:client_idx = 9, local_sample_number = 5345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:seed: 0!!!!!!!\n",
      "INFO:root:client_idx = 9, batch_num_train_local = 84, batch_num_test_local = 16\n",
      "Initializing clients...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:q3i1bnhy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15753... <strong style=\"color:red\">(failed 2).</strong> Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/fw/anaconda3/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">FedAVG(d)SNIP-target_kr0.1-0.1-classic_iid-prune_at_1rTrue-single_shot_pruningTrue-lr0.01-clip_gradFalse</strong>: <a href=\"https://wandb.ai/slimfun/fedsnip/runs/q3i1bnhy\" target=\"_blank\">https://wandb.ai/slimfun/fedsnip/runs/q3i1bnhy</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220916_135703-q3i1bnhy/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:q3i1bnhy). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.3 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/slimfun/fedsnip/runs/20fegk0p\" target=\"_blank\">FedAVG(d)Iter-SNIP-target_kr0.1-0.1-classic_iid-prune_at_1rTrue-single_shot_pruningTrue-lr0.01-clip_gradFalse</a></strong> to <a href=\"https://wandb.ai/slimfun/fedsnip\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server model param size: 69007680\n",
      "keep_ratio_steps: [0.55, 0.09999999999999998]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15623/788602005.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfedsnip_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/FL/efficient_FL/feddst/fedsnip.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'keep_ratio_steps: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_ratio_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mlocal_sparsity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkeep_ratio\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeep_ratio_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "run = fedsnip_obj.main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fff754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# print('...')\n",
    "# name = input()\n",
    "# print('++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88461634",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     debug_info = next(run)\n",
    "#     print(debug_info.msg)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     input()\n",
    "debug_info = next(run)\n",
    "masks = debug_info.obj\n",
    "# global_model = debug_info.obj\n",
    "# aggregated_masks = debug_info.obj[0]\n",
    "# cl_mask_prarms = debug_info.obj[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea00208",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = server.masks\n",
    "torch.cat(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = masks\n",
    "def count_vote(masks, vote):\n",
    "    tc = 0.\n",
    "    keeped = 0.\n",
    "    for i in range(len(masks)):\n",
    "        tc += masks[i].numel()\n",
    "        m = masks[i].clone().detach()\n",
    "        keeped += torch.sum(torch.where(m >= vote, 1, 0))\n",
    "\n",
    "    print(\"keeped: {}, total: {}, pct: {}\".format(keeped, tc, keeped/tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed56025",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('vote {}'.format(i))\n",
    "    count_vote(server.masks, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = server.masks\n",
    "for i in range(len(masks)):\n",
    "    print(masks[i].shape)\n",
    "flat_masks = torch.cat([m.flatten() for m in masks])\n",
    "keep_num = len(flat_masks) * 0.1\n",
    "threshold, indices = flat_masks.topk(int(keep_num))\n",
    "global_masks = torch.zeros_like(flat_masks)\n",
    "global_masks[indices] = 1\n",
    "print(indices.sort())\n",
    "# len(indices)\n",
    "print(global_masks[:25])\n",
    "print(global_masks.sum())\n",
    "\n",
    "idx = 0\n",
    "ms = []\n",
    "for i in range(len(masks)):\n",
    "    m = global_masks[idx:idx+masks[i].numel()].reshape(masks[i].size())\n",
    "    idx += masks[i].numel()\n",
    "    print(m.shape)\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e089e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5])\n",
    "c = torch.tensor([6,7,8,9])\n",
    "ts = torch.cat([e.flatten() for e in [a,b,c]])\n",
    "print(ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba2be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_c = 0.0\n",
    "total = 0.0\n",
    "\n",
    "for name, param in server.model.state_dict().items():\n",
    "    a = param.view(-1).to(device='cpu', copy=True).numpy()\n",
    "    pruned_c +=sum(np.where(a, 0, 1))\n",
    "    total += param.numel()\n",
    "print(f'global model zero params: {pruned_c / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8968b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in server.model.state_dict():\n",
    "#     print(name)\n",
    "server.model = server.model.to('cuda:3')\n",
    "params = server.model.cpu().state_dict()\n",
    "print(params['features.0.weight'])\n",
    "\n",
    "print('***********'*3)\n",
    "\n",
    "# params['features.0.weight'] = torch.zeros_like(params['features.0.weight'])\n",
    "params['features.0.weight'][0][0][0] = 2.\n",
    "\n",
    "print(server.model.state_dict()['features.0.weight'])\n",
    "\n",
    "\n",
    "print(params['features.0.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d24b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_c = 0.0\n",
    "total = 0.0\n",
    "for name, param in global_model.state_dict().items():\n",
    "    a = param.view(-1).to(device='cpu', copy=True).numpy()\n",
    "    pruned_c +=sum(np.where(a, 0, 1))\n",
    "    total += param.numel()\n",
    "print(f'global model zero params: {pruned_c / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27829812",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_c = 0.\n",
    "total = 0.\n",
    "for name,mask in global_model.mask.items():\n",
    "#     print(mask)\n",
    "    prune_c += sum(np.where(mask.to('cpu', copy=True).view(-1).numpy(), 0, 1))\n",
    "    total += mask.numel()\n",
    "    \n",
    "print(prune_c / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d7b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb36d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "pruned_c = 0.0\n",
    "total = 0.0\n",
    "for name, param in global_model.state_dict().items():\n",
    "    a = param.view(-1).to(device='cpu', copy=True).numpy()\n",
    "    pruned_c +=sum(np.where(a, 0, 1))\n",
    "    total += param.numel()\n",
    "print(f'global model zero params: {pruned_c / total}')\n",
    "\n",
    "gcp_model = copy.deepcopy(global_model)\n",
    "\n",
    "\n",
    "\n",
    "prune_c = 0.\n",
    "total = 0.\n",
    "for name, params in gcp_model.state_dict().items():\n",
    "#     print(name)\n",
    "    prune_c += sum(np.where(params.to('cpu', copy=True).view(-1).numpy(), 0, 1))\n",
    "    total += params.numel()\n",
    "    \n",
    "print(prune_c / total)\n",
    "    \n",
    "# print('*'*10)\n",
    "# for name, mask in global_model.mask.items():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bca0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in aggregated_masks:\n",
    "#     print(name)\n",
    "#     print(aggregated_masks[name].dtype)\n",
    "\n",
    "def count_mask(state_dict):\n",
    "    non_zero = 0.\n",
    "    total = 0.\n",
    "    for name in state_dict:\n",
    "        non_zero += torch.count_nonzero(state_dict[name])\n",
    "        total += state_dict[name].numel()\n",
    "    return 1 - non_zero / total\n",
    "# a = aggregated_masks['features.4.weight']\n",
    "print(a.shape)\n",
    "# print(torch.count_nonzero(a))\n",
    "print(count_mask(aggregated_masks))\n",
    "mask = torch.where(a>=1, 1, 0)\n",
    "# print(torch.count_nonzero(mask))\n",
    "print(count_mask(cl_mask_prarms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "keep_masks = {0: [torch.Tensor([0, 1, 0, 1, 1])], 1: [torch.Tensor([1, 1, 0, 0, 1])]}\n",
    "\n",
    "model_list = [[2, torch.Tensor([1,2,3,4,5])], [3, torch.Tensor([1,2,3,4,5])]]\n",
    "\n",
    "masks = copy.deepcopy(keep_masks)\n",
    "for c, m in masks.items():\n",
    "    for i in range(len(m)):\n",
    "        m[i] *= model_list[c][0]\n",
    "    \n",
    "print(masks)\n",
    "\n",
    "total_masks = copy.deepcopy(masks)\n",
    "for c,m in total_masks.items():\n",
    "    if c!= 0:\n",
    "        for i in range(len(m)):\n",
    "            total_masks[0][i] += m[i]\n",
    "        \n",
    "print(total_masks)\n",
    "\n",
    "for c, m in masks.items():\n",
    "    for i in range(len(m)):\n",
    "        m[i] /= total_masks[0][i]\n",
    "        masks[c][i] = torch.where(torch.isnan(m[i]), torch.full_like(m[i], 0), m[i])\n",
    "#     print(torch.isnan(m))\n",
    "print(masks)\n",
    "#     m /= sum(model_list[i][0] for i in range(len(model_list)))\n",
    "\n",
    "# print(masks[0] * model_list[0][1])\n",
    "model_list[0][1] *= masks[0]\n",
    "# print(model_list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb781364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "list_tensor = [torch.Tensor([1,2]), torch.Tensor([2,3])]\n",
    "t = torch.stack(list_tensor)\n",
    "list_tensor *= 2\n",
    "list_tensor\n",
    "# print(t)\n",
    "# a = torch.from_numpy(np.asarray(list_tensor))\n",
    "# print(a)\n",
    "# torch.cat(list_tensor, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fc21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([2,3]).type(torch.float64)\n",
    "b = torch.Tensor([1,2]).type(torch.float64)\n",
    "print(a.dtype == b.dtype)\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(6, 2, bias=False),\n",
    "          nn.Sigmoid(),\n",
    "        )\n",
    "input = torch.randn(6)\n",
    "target = torch.randn(2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.1)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "output = model(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "print(list(model.parameters())[0].grad)\n",
    "optimizer.zero_grad() \n",
    "print(list(model.parameters())[0].grad)\n",
    "\n",
    "\n",
    "print(\"before step: \", list(model.parameters()))\n",
    "optimizer.step()\n",
    "print(\"after step: \", list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911d5cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_keep_ratio = 0.2\n",
    "num_pruning_steps = 2\n",
    "\n",
    "keep_ratio_steps = [1 - ((x + 1) * (1 - target_keep_ratio) / num_pruning_steps) for x in range(num_pruning_steps)]\n",
    "keep_ratio_steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
